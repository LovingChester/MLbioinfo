{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f4f2022-fce8-4abe-bbd1-fbeeca0ee8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55cd24d7-82c0-48f9-b07f-736533a279ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegSampler:\n",
    "    '''generate a block of negative samples from the cumsum array (Cumulative Distribution Function)'''\n",
    "    def __init__(self, csum_ary):\n",
    "        self.csum_ary = csum_ary\n",
    "        self.time = 0.\n",
    "\n",
    "    def get_neg_words(self, num_words):\n",
    "        '''get num_words negative words sampled from cumsum array'''\n",
    "        st_time = time.time()\n",
    "        nprobs = np.random.random(num_words)\n",
    "        neg_words = np.searchsorted(self.csum_ary, nprobs)\n",
    "        en_time = time.time()\n",
    "        self.time += en_time-st_time\n",
    "        return neg_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cdd459c-08cf-4a60-b1b7-02764bcb2b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequences:\n",
    "    '''This class reads in the sequences, creates the vocab, and then acts as an iterator class\n",
    "    to return tokenized sequences for each offset'''\n",
    "    def __init__(self, filename, kmer):\n",
    "        '''read protein sequences from file'''\n",
    "        self.kmer = kmer  # which kmer size?\n",
    "        self.vocab_pow = 0.75 #rescale high-freq words\n",
    "        \n",
    "        self.sequences = []  # set of sequences\n",
    "        self.alphabet = {}  # set of characters/symbols\n",
    "        self.totlen = 0 # total length over all sequences\n",
    "        with open(filename, \"r\") as f:\n",
    "            for line in f.readlines():\n",
    "                a = line.strip()\n",
    "                if a[0] == \"#\":\n",
    "                    continue\n",
    "                elif a == \"sequence\":\n",
    "                    continue\n",
    "                else:\n",
    "                    self.sequences.append(a)\n",
    "                    self.totlen += len(a)\n",
    "                    for c in a:\n",
    "                        self.alphabet[c] = True\n",
    "                        \n",
    "        self.idxs = np.arange(len(self.sequences)) # index array for shuffling\n",
    "        # distinct chars/AA\n",
    "        self.alphabet = sorted(self.alphabet.keys())\n",
    "        \n",
    "        # vocab will be defined over kmers\n",
    "        self.create_vocab()\n",
    "        \n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        '''define iterator for tokenized sequences into kmers'''\n",
    "        self.sidx = 0\n",
    "         #shuffle the sequences\n",
    "        np.random.shuffle(self.idxs)\n",
    "        self.orf = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        '''tokenize the sequences into kmers and\n",
    "        return sequences with different offsets.\n",
    "        also make sure to map kmers/words to index'''\n",
    "        if self.sidx < len(self.sequences):\n",
    "            seq = self.sequences[self.idxs[self.sidx]] # return sequences in shuffled order\n",
    "            tokenized_seq = [\n",
    "                self.word_to_idx[seq[i : i + self.kmer]]\n",
    "                for i in range(self.orf, len(seq) - self.kmer + 1, self.kmer)\n",
    "            ]\n",
    "            if self.orf == self.kmer - 1:\n",
    "                self.orf = 0\n",
    "                self.sidx += 1\n",
    "            else:\n",
    "                self.orf += 1\n",
    "            return tokenized_seq\n",
    "        else:\n",
    "            raise StopIteration\n",
    "\n",
    "    def create_vocab(self):\n",
    "        '''how many kmers are there in the protein sequences\n",
    "        compute the freq of each word, and convert to prob dist'''\n",
    "        self.vocab_freq = defaultdict(float)\n",
    "        siter = iter(self)\n",
    "        st_time = time.time()\n",
    "        for seq in tqdm(self.sequences):\n",
    "            # compute freq of each word\n",
    "            for i in range(len(seq)-self.kmer+1):\n",
    "                    self.vocab_freq[seq[i:i+self.kmer]] += 1.\n",
    "     \n",
    "        \n",
    "        self.vocab = sorted(self.vocab_freq.keys())\n",
    "        \n",
    "        # create forward and reverse index for all the words in vocab\n",
    "        self.word_to_idx = {w: idx for (idx, w) in enumerate(self.vocab)}\n",
    "        self.idx_to_word = {idx: w for (idx, w) in enumerate(self.vocab)}   \n",
    "        \n",
    "        # convert to prob dist after raising to 0.75 power, then compute cumsum\n",
    "        self.vocab_prob = np.array([self.vocab_freq[k] for k in self.vocab])\n",
    "        self.vocab_prob **= self.vocab_pow\n",
    "        total_freq = self.vocab_prob.sum()\n",
    "        self.vocab_prob /= self.vocab_prob.sum()\n",
    "        self.vocab_csum = np.cumsum(self.vocab_prob)\n",
    "        \n",
    "        en_time = time.time()\n",
    "        print(\"vocab time\", en_time - st_time, total_freq)\n",
    "\n",
    "    def print_vocab(self):\n",
    "        '''print the vocab'''\n",
    "        print(self.alphabet)\n",
    "        for i, (k, f) in enumerate(zip(self.vocab, self.vocab_freq)):\n",
    "            print(i, k, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e78e9a2-4f3b-4087-82f4-590d8bc812ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosNegSampler(torch.utils.data.IterableDataset):\n",
    "    '''This class creates a block of positive and negative pairs for word2vec training\n",
    "    The iterable will return a numpy array of target, context and label triples\n",
    "    It uses multiple workers to speed up the batch generation'''\n",
    "    \n",
    "    def __init__(self, S, window_size, neg_samples, block_size, num_workers):\n",
    "        '''\n",
    "        S: Sequences class instance\n",
    "        window_size: this is context_size//2; how many to left and right of target\n",
    "        neg_samples: how many negative samples per positive\n",
    "        block_size: how many pos and negative pairs (plus labels) per block\n",
    "        num_workers: how many workers\n",
    "        '''\n",
    "        super(PosNegSampler, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.neg_samples = neg_samples\n",
    "        self.time = 0.\n",
    "        self.block_sz = block_size\n",
    "        \n",
    "        #how many workers\n",
    "        if num_workers <= 0:\n",
    "           self.num_workers = 1\n",
    "        else:\n",
    "           self.num_workers = num_workers\n",
    "        \n",
    "        self.S = S\n",
    "        \n",
    "        # create one neg_sampler class per worker (for better parallelism)\n",
    "        self.neg_sampler = [None] * self.num_workers\n",
    "        for i in range(self.num_workers):\n",
    "           self.neg_sampler[i] = NegSampler(self.S.vocab_csum)\n",
    "                    \n",
    "    def context_data(self, block_sz, worker_id):\n",
    "        '''generate center word, context word pairs\n",
    "           generate one block per worker\n",
    "        '''\n",
    "        T = [list()] * self.num_workers\n",
    "        C = [list()] * self.num_workers\n",
    "        for k, seq in enumerate(iter(self.S)):\n",
    "            # each worker picks one of the sequences in round-robin\n",
    "            if k % self.num_workers == worker_id:\n",
    "                for i, word in enumerate(seq):\n",
    "                    start_idx = max(0, i - self.window_size)\n",
    "                    end_idx = min(len(seq), i + self.window_size + 1)\n",
    "                    for j in range(start_idx, end_idx):\n",
    "                        if i != j:\n",
    "                            T[worker_id].append(word)\n",
    "                            C[worker_id].append(seq[j])\n",
    "                    # return a block of T, C\n",
    "                    if len(T[worker_id]) >= block_sz:\n",
    "                        yield (T[worker_id], C[worker_id])\n",
    "                        T[worker_id], C[worker_id] = [], []\n",
    "                        \n",
    "        # return any remining elements\n",
    "        for id in range(self.num_workers):            \n",
    "            yield (T[id], C[id])\n",
    "                    \n",
    "    def __len__(self):\n",
    "        ''' approx # of w,c pairs '''\n",
    "        self.wc_pairs = 2 * self.window_size * (self.S.totlen - self.S.kmer + 1)\n",
    "        self.wc_pairs *= self.neg_samples + 1\n",
    "        self.wc_pairs /= self.block_sz\n",
    "        self.wc_pairs = int(self.wc_pairs)\n",
    "        return self.wc_pairs\n",
    "\n",
    "    def __iter__(self):\n",
    "        '''return one pos word and neq_samples neg words and the labels\n",
    "           use context_data to retrieve a block\n",
    "        '''\n",
    "        worker = data.get_worker_info()\n",
    "        if worker is not None:\n",
    "           worker_id = worker.id\n",
    "           num_workers = worker.num_workers\n",
    "        else:\n",
    "           worker_id = 0\n",
    "           num_workers = 1\n",
    "\n",
    "        st_time = time.time()\n",
    "        \n",
    "        for i, (T, C) in enumerate(self.context_data(self.block_sz, worker_id)):\n",
    "            Tnp = np.array(T)\n",
    "            Cnp = np.array(C)\n",
    "            L = np.ones(len(T))\n",
    "            yield (Tnp, Cnp, L)\n",
    "            for j in range(self.neg_samples):\n",
    "                L = np.zeros(len(T))\n",
    "                N = self.neg_sampler[worker_id].get_neg_words(\n",
    "                    len(T))\n",
    "                Nnp = np.array(N)\n",
    "                yield (Tnp, Nnp, L)\n",
    "                \n",
    "        en_time = time.time()\n",
    "        self.time += en_time-st_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ba73e96-9d1c-4eaf-8543-eb595d6dcad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prot2Vec_NegSampling(nn.Module):\n",
    "    '''The word2vec model to train the kmer embeddings'''\n",
    "    def __init__(self, embedding_size, vocab_size):\n",
    "        super(Prot2Vec_NegSampling, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.T = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.C = nn.Embedding(vocab_size, embedding_size)\n",
    "\n",
    "    def forward(self, target_word, context_word, label):\n",
    "        t = self.T(target_word)\n",
    "        c = self.C(context_word)\n",
    "        out = torch.sum(t * c, dim=1)\n",
    "        return out\n",
    "\n",
    "    def save_embeddings(self, file_name, idx_to_word, mode):\n",
    "        if mode == 'avg':\n",
    "            # average the T and C matrices\n",
    "            W = (net.T.weight.cpu().data.numpy() + net.C.weight.cpu().data.numpy())/2.\n",
    "        elif mode == 'target':\n",
    "            # W is T matrices\n",
    "            W = net.T.weight.cpu().data.numpy()\n",
    "        elif mode == 'context':\n",
    "            # W is C matrices\n",
    "            W = net.C.weight.cpu().data.numpy()            \n",
    "        else:\n",
    "            sys.exit(f'{mode} not supported')\n",
    "        \n",
    "        with open(file_name, \"w\") as f:\n",
    "            f.write(\"%d %d\\n\" % (len(idx_to_word), self.embedding_size))\n",
    "            for wid, w in idx_to_word.items():\n",
    "                e = ' '.join(map(lambda x: str(x), W[wid]))\n",
    "                f.write(\"%s %s\\n\" % (w, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52c89c15-0f42-45e3-b53e-23c07fd139ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='word2vec.py')\n",
    "    parser.add_argument('-f', dest='fname')\n",
    "    parser.add_argument('-d', default=100, type=int)\n",
    "    parser.add_argument(\n",
    "        '-w',\n",
    "        dest='context_size',\n",
    "        default=25,\n",
    "        type=int,\n",
    "        help='context size must be odd and >=3',\n",
    "    )\n",
    "    parser.add_argument('-k', dest='kmer', default=3, type=int)\n",
    "    parser.add_argument('-q', dest='neg_samples', default=5, type=int)\n",
    "    parser.add_argument('-e', dest='epochs', default=10, type=int)\n",
    "    parser.add_argument('-nw', dest='num_workers', default=0, type=int)\n",
    "    parser.add_argument('-b', dest='batch_size', default=1, type=int)\n",
    "    parser.add_argument('-B', dest='block_size', default=512*1024, type=int)\n",
    "    parser.add_argument('-lr', dest='learning_rate', default=0.01, type=float)\n",
    "    parser.add_argument('-j', dest='jobid', default=1, type=int)\n",
    "    parser.add_argument('-D', dest='device', default=0, type=int)\n",
    "\n",
    "    # set the input args for running the code\n",
    "    args = parser.parse_args(\"-f uniprot-reviewed-lim_sequences.txt -lr 0.01 -e 5 -nw 4\".split())\n",
    "    \n",
    "    if args.context_size % 2 == 0 or args.context_size < 3:\n",
    "        sys.exit(\"context size must always be odd and at least 3\")\n",
    "    args.window_size = args.context_size // 2  # context = 2*window+1\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b143c3-f1e1-45a8-99d9-dd0a351882ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(fname='uniprot-reviewed-lim_sequences.txt', d=100, context_size=25, kmer=3, neg_samples=5, epochs=5, num_workers=4, batch_size=1, block_size=524288, learning_rate=0.01, jobid=1, device=0, window_size=12)\n",
      "using device Tesla T4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 524529/524529 [00:41<00:00, 12582.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab time 41.694188833236694 13158012.444679776\n",
      "vocab, alphabet, device:  10150 25 cuda:0 46967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1558/46967 [01:06<35:03, 21.59it/s]"
     ]
    }
   ],
   "source": [
    "# Main training wrapper code\n",
    "args = parse_args()\n",
    "print(args)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = f\"cuda:{args.device}\"\n",
    "    print(\"using device\", torch.cuda.get_device_name(device))\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    \n",
    "# read sequences, create dataset\n",
    "S = Sequences(args.fname, args.kmer)\n",
    "PNS = PosNegSampler(S, args.window_size, args.neg_samples, args.block_size, args.num_workers)\n",
    "V = len(PNS.S.vocab)\n",
    "print(\"vocab, alphabet, device: \", V, len(PNS.S.alphabet), device, len(PNS))\n",
    "\n",
    "training_generator = data.DataLoader(\n",
    "    PNS, batch_size=args.batch_size, num_workers=args.num_workers\n",
    ")\n",
    "\n",
    "# create the NN model\n",
    "net = Prot2Vec_NegSampling(embedding_size=args.d, vocab_size=V)\n",
    "net.to(device)\n",
    "\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=args.learning_rate)\n",
    "\n",
    "\n",
    "ckpt_fname = f'ckpt_J{args.jobid}.pth'\n",
    "\n",
    "start_t = time.time()\n",
    "for e in range(args.epochs):\n",
    "    running_loss = 0\n",
    "    for bidx, (targets, contexts, labels) in enumerate(\n",
    "        tqdm(training_generator, total=len(PNS))):\n",
    "        \n",
    "        targets = targets.flatten().to(device)\n",
    "        contexts = contexts.flatten().to(device)\n",
    "        labels = labels.flatten().to(device)\n",
    "        \n",
    "        net.zero_grad()\n",
    "        preds = net(targets, contexts, labels)\n",
    "        loss = loss_function(preds, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if bidx % 100 == 0:\n",
    "            checkpoint = {\n",
    "                'batch': bidx,\n",
    "                'epoch': e,\n",
    "                'loss': running_loss,\n",
    "                'state_dict': net.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "            }\n",
    "            torch.save(checkpoint, ckpt_fname)\n",
    "\n",
    "    print(\"epoch\", e, running_loss, bidx, running_loss / (bidx + 1))\n",
    "    checkpoint = {\n",
    "        'batch': bidx,\n",
    "        'epoch': e,\n",
    "        'loss': running_loss,\n",
    "        'state_dict': net.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, ckpt_fname)\n",
    "    \n",
    "end_t = time.time()\n",
    "print(\"finished in time\", end_t - start_t, args.num_workers)\n",
    "\n",
    "for mode in ['avg', 'target', 'context']:\n",
    "    output_file = \"prot_embeddings_m%s_k%s_w%s_q%s_d%s_lr%s_J%s.vec\" % (\n",
    "        mode,\n",
    "        str(args.kmer),\n",
    "        str(args.context_size),\n",
    "        str(args.neg_samples),\n",
    "        str(args.d),\n",
    "        str(args.learning_rate),\n",
    "        args.jobid,\n",
    "    )\n",
    "    net.save_embeddings(output_file, PNS.S.idx_to_word, mode)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
