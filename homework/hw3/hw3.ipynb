{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db83b620-6060-427a-bdda-e2f90b24a686",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b5ad850-c40d-4c74-bb86-d487d5f66f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as du\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47994fa-c86c-447e-86d4-7f56e3d1117f",
   "metadata": {},
   "source": [
    "#### Create Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b1e8d27-d397-4c47-a166-b0aaf2becea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15625\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "# open the file and read\n",
    "#file = open(\"small_uniprot.txt\", \"r\")\n",
    "file = open(\"uniprot-reviewed-lim_sequences.txt\", \"r\")\n",
    "\n",
    "# skip the first line\n",
    "file.readline()\n",
    "sequences = file.read().rstrip()\n",
    "letters = sorted(set(sequences.replace('\\n','')))\n",
    "\n",
    "# k-mer, n_gram size, k\n",
    "n_gram = 3\n",
    "# refer to window size w\n",
    "context_size = 25\n",
    "# negative sampling size q\n",
    "neg_sample = 5\n",
    "# get all permutations of letters\n",
    "vocabs = list(map(lambda x: ''.join(x), list(product(letters, repeat=n_gram))))\n",
    "print(len(vocabs))\n",
    "# give each vocab a index\n",
    "vocab_to_idx = {vocab: index+1 for index, (vocab) in enumerate(vocabs)}\n",
    "print(vocab_to_idx['AAA'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8525c8-5e28-4bb5-9d1f-df14cef333be",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Create Dataset Class (Batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "832ecff1-f2a0-4aab-857a-d93bc4d72880",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "# numpy searchsorted\n",
    "class SEQPAIR_Dataset(Dataset):\n",
    "    def __init__(self):\n",
    "        super(SEQPAIR_Dataset, self).__init__()\n",
    "        \n",
    "        self.word_to_idx = dict()\n",
    "        self.idx_to_word = dict()\n",
    "        self.seq_to_ngrams = []\n",
    "        self.word_to_prob = dict()\n",
    "        self.CPD = []\n",
    "        \n",
    "        self.generate_ngrams(0)\n",
    "        self.generate_ngrams(1)\n",
    "        self.generate_ngrams(2)\n",
    "        print(len(self.word_to_idx.keys()))\n",
    "        \n",
    "        for word in sorted(self.word_to_prob.keys()):\n",
    "            self.CPD.append(self.word_to_prob[word] ** 0.75)\n",
    "        \n",
    "        self.CPD = np.array(self.CPD) / np.sum(self.CPD)\n",
    "        self.CPD = np.cumsum(self.CPD)\n",
    "    \n",
    "    def generate_ngrams(self, offset=0):\n",
    "        \n",
    "        for sequence in sequences.split('\\n'):\n",
    "            n_grams = []\n",
    "            begin = offset\n",
    "            end = begin + n_gram\n",
    "            while(end <= len(sequence)):\n",
    "                word = sequence[begin:end]\n",
    "                n_grams.append(vocab_to_idx[word])\n",
    "                self.word_to_idx[word] = vocab_to_idx[word]\n",
    "                self.idx_to_word[vocab_to_idx[word]] = word\n",
    "                if word not in self.word_to_prob:\n",
    "                    self.word_to_prob[word] = 1\n",
    "                else:\n",
    "                    self.word_to_prob[word] += 1\n",
    "                begin += n_gram\n",
    "                end += n_gram\n",
    "            self.seq_to_ngrams.append(n_grams)\n",
    "    \n",
    "    def __len__(self):\n",
    "        '''return len of dataset'''\n",
    "        return len(self.seq_to_ngrams)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        n_grams = self.seq_to_ngrams[idx]\n",
    "        size = len(n_grams)\n",
    "        #space = size * (context_size-1) * (neg_sample+1)\n",
    "        target_words = np.zeros((size,1)).astype(np.int_)\n",
    "        context_words = np.zeros((size, context_size)).astype(np.int_)\n",
    "        neg_words = np.zeros((size, context_size*neg_sample)).astype(np.int_)\n",
    "        incre = (context_size-1) * (neg_sample+1)\n",
    "        begin = 0\n",
    "        end = begin + incre\n",
    "        for i in range(0, size):\n",
    "            \n",
    "            target_words[i,0] = n_grams[i]\n",
    "            leftbound = max(i-int(context_size/2), 0)\n",
    "            left_size = i - leftbound\n",
    "            rightbound = min(i+int(context_size/2), size)\n",
    "            right_size = rightbound - i\n",
    "            context_words[i, 0:left_size] = n_grams[leftbound:i]\n",
    "            context_words[i, left_size:left_size+right_size] = n_grams[i:rightbound]\n",
    "            prob = np.random.rand(context_size*neg_sample)\n",
    "            pos = np.searchsorted(self.CPD, prob)\n",
    "            neg_words[i,:] = pos.astype(np.int_)\n",
    "\n",
    "        return target_words, context_words, neg_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d524c5b-97cc-411b-9399-7e8576d24e69",
   "metadata": {},
   "source": [
    "#### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3b7a5eb-6e42-4bd5-872d-4d252b5110c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinEmbeddingModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        \n",
    "        super(ProteinEmbeddingModel, self).__init__()\n",
    "        \n",
    "        # define target embedding layer\n",
    "        self.target = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=0, sparse=True)\n",
    "        \n",
    "        # define context embedding layer\n",
    "        self.context = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=0, sparse=True)\n",
    "        \n",
    "    def forward(self, target_words, context_words, neg_words):\n",
    "        #print('enter')\n",
    "        t = target_words[0]\n",
    "        # get the target representation\n",
    "        t = self.target(t)\n",
    "        #print(t)\n",
    "        \n",
    "        c = context_words[0]\n",
    "        # get the context representation\n",
    "        c = self.context(c)\n",
    "        c = torch.transpose(c, 1, 2)\n",
    "        #print(c)\n",
    "        \n",
    "        n = neg_words[0]\n",
    "        #print(n)\n",
    "        n = self.context(n)\n",
    "        n = torch.transpose(n, 1, 2)\n",
    "        \n",
    "        # calculate the dot product\n",
    "        #x = torch.sum(t * c, 1)\n",
    "        positive = torch.matmul(t, c).reshape(-1)\n",
    "        negative = torch.matmul(t, n).reshape(-1)\n",
    "        \n",
    "        return torch.cat((positive, negative), 0), torch.cat((torch.ones(len(positive)), torch.zeros(len(negative))), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb5a499-bc78-4e96-abc0-32a411704692",
   "metadata": {},
   "source": [
    "#### Set Up Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec1673f0-7115-4193-946f-66de9536f878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda:0\n",
      "10150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ProteinEmbeddingModel(\n",
       "  (target): Embedding(15625, 100, padding_idx=0, sparse=True)\n",
       "  (context): Embedding(15625, 100, padding_idx=0, sparse=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = f'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"using device: {device}\")\n",
    "batch_size = 1\n",
    "learning_rate = 0.05\n",
    "epochs = 1\n",
    "\n",
    "embedding_dim = 100\n",
    "model = ProteinEmbeddingModel(len(vocab_to_idx)+1, embedding_dim)\n",
    "optimizer = optim.SparseAdam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "data = SEQPAIR_Dataset()\n",
    "train_loader = du.DataLoader(dataset=data,\n",
    "                        batch_size=batch_size,\n",
    "                        collate_fn=None,\n",
    "                        shuffle=True, num_workers=4)\n",
    "\n",
    "model = model.to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9786cdf2-e65c-4a3d-b4e2-21716c3cd283",
   "metadata": {},
   "source": [
    "#### Training Loop Over Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cae8bc2-84fa-4bb3-b6eb-341dca69e092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "400000\n",
      "410000\n",
      "420000\n",
      "430000\n",
      "440000\n",
      "450000\n",
      "460000\n",
      "470000\n",
      "480000\n",
      "490000\n",
      "500000\n",
      "510000\n",
      "520000\n",
      "530000\n",
      "540000\n",
      "550000\n",
      "560000\n",
      "570000\n",
      "580000\n",
      "590000\n",
      "600000\n",
      "610000\n",
      "620000\n",
      "630000\n",
      "640000\n",
      "650000\n",
      "660000\n",
      "670000\n",
      "680000\n",
      "690000\n",
      "700000\n",
      "710000\n",
      "720000\n",
      "730000\n",
      "740000\n",
      "750000\n",
      "760000\n",
      "770000\n",
      "780000\n",
      "790000\n",
      "800000\n",
      "810000\n",
      "820000\n",
      "830000\n",
      "840000\n",
      "850000\n",
      "860000\n",
      "870000\n",
      "880000\n",
      "890000\n",
      "900000\n",
      "910000\n",
      "920000\n",
      "930000\n",
      "940000\n",
      "950000\n",
      "960000\n",
      "970000\n",
      "980000\n",
      "990000\n",
      "1000000\n",
      "1010000\n",
      "1020000\n",
      "1030000\n",
      "1040000\n",
      "1050000\n",
      "1060000\n",
      "1070000\n",
      "1080000\n",
      "1090000\n",
      "1100000\n",
      "1110000\n",
      "1120000\n",
      "1130000\n",
      "1140000\n",
      "1150000\n",
      "1160000\n",
      "1170000\n",
      "1180000\n",
      "1190000\n",
      "1200000\n",
      "1210000\n",
      "1220000\n",
      "1230000\n",
      "1240000\n",
      "1250000\n",
      "1260000\n",
      "1270000\n",
      "1280000\n",
      "1290000\n",
      "1300000\n",
      "1310000\n",
      "1320000\n",
      "1330000\n",
      "1340000\n",
      "1350000\n",
      "1360000\n",
      "1370000\n",
      "1380000\n",
      "1390000\n",
      "1400000\n",
      "1410000\n",
      "1420000\n",
      "1430000\n",
      "1440000\n",
      "1450000\n",
      "1460000\n",
      "1470000\n",
      "1480000\n",
      "1490000\n",
      "1500000\n",
      "1510000\n",
      "1520000\n",
      "1530000\n",
      "1540000\n",
      "1550000\n",
      "1560000\n",
      "1570000\n",
      "Epoch: 1, Loss: 0.239633, time used: 122.591\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    start = time.time()\n",
    "    sum_loss = 0.\n",
    "    for batch_idx, (target_words, context_words, neg_words) in enumerate(train_loader):\n",
    "        # print(target_words)\n",
    "        # print(target_words.size())\n",
    "        # print(context_words)\n",
    "        # print(context_words.size())\n",
    "        #print(batch_idx)\n",
    "        #break\n",
    "        # send batch over to device\n",
    "        target_words, context_words, neg_words = target_words.to(device), context_words.to(device), neg_words.to(device)\n",
    "\n",
    "        # zero out prev gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # run the forward pass\n",
    "        output, label = model(target_words, context_words, neg_words)\n",
    "        #break\n",
    "        \n",
    "        label = label.to(device)\n",
    "        # compute loss/error\n",
    "        #label.type(torch.float)\n",
    "        loss = F.binary_cross_entropy_with_logits(output, label)\n",
    "\n",
    "        # sum up batch losses\n",
    "        sum_loss += loss.item()\n",
    "\n",
    "        # compute gradients and take a step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 10000 == 0:\n",
    "            print(batch_idx)\n",
    "\n",
    "    # average loss per example\n",
    "    sum_loss /= len(train_loader)\n",
    "    end = time.time()\n",
    "    time_used = (end - start) / 60\n",
    "    print(f'Epoch: {epoch}, Loss: {sum_loss:.6f}, time used: {time_used:.3f}')\n",
    "    \n",
    "    # model save\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'loss': sum_loss,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict()}\n",
    "    torch.save(checkpoint, 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d71f7f-c7e6-4bbc-a57d-2b9b7535182c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model, 'ProteinEmbeddingModel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14663386-6d9a-48f1-adc4-044032327572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 1., 1.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a = np.zeros(5)\n",
    "# a[2:5] = np.array([1]*3)\n",
    "# a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2baca089-9466-4399-a919-fb5f911d8fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.3285,  1.0026, -0.9661]],\n",
      "\n",
      "        [[ 0.9846, -0.3995, -1.2737]]], grad_fn=<EmbeddingBackward0>)\n",
      "tensor([[[-2.0648, -2.0181,  1.0010,  1.6419],\n",
      "         [-0.0215, -1.3848, -1.3466,  0.0663],\n",
      "         [-0.8005,  1.2505, -1.9501, -0.4806]],\n",
      "\n",
      "        [[ 1.0010, -0.1660, -2.0181,  0.1638],\n",
      "         [-1.3466,  0.9991, -1.3848, -0.1109],\n",
      "         [-1.9501, -0.1595,  1.2505, -0.1939]]], grad_fn=<TransposeBackward0>)\n",
      "tensor([[[-1.9913, -5.2775,  1.8636,  2.7119]],\n",
      "\n",
      "        [[ 4.0073, -0.3594, -3.0265,  0.4526]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-1.9913, -5.2775,  1.8636,  2.7119,  4.0073, -0.3594, -3.0265,  0.4526],\n",
       "       grad_fn=<ReshapeAliasBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding = nn.Embedding(10, 3)\n",
    "# # a batch of 2 samples of 4 indices each\n",
    "# input = torch.LongTensor([[1,2,4,5],[4,3,2,9]])\n",
    "# x = torch.LongTensor([[6],[8]])\n",
    "# x_e = embedding(x)\n",
    "# print(x_e)\n",
    "# #print(embedding(input))\n",
    "# z = torch.transpose(embedding(input), 1, 2)\n",
    "# print(z)\n",
    "\n",
    "# print(torch.matmul(x_e, z))\n",
    "# torch.matmul(x_e, z).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c538f48c-25fb-4386-8ddd-63b8b8fd8247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.array([1,2]).astype(np.int_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c244dc9-1315-49c6-ab23-67a62f92c888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 0., 0., 0.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.cat((torch.ones(3), torch.zeros(3)), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120299bd-606a-47cc-aeeb-e5786853a99d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
