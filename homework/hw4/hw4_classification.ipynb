{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "656a1a72-bf03-401f-83ff-94ba9af50145",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b0b4112-dcd0-4204-92fb-797e22add711",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from torch.utils import data\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4b6636-9d17-4d28-a426-a563abd7d1d1",
   "metadata": {},
   "source": [
    "#### The BERT Model For Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c197b65d-cde4-4611-8936-37aea753e838",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleAttention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        \n",
    "        super(SingleAttention, self).__init__()\n",
    "        \n",
    "        self.d_k = int(d_model / 8)\n",
    "        \n",
    "        self.W_Q = nn.Linear(d_model, self.d_k)\n",
    "        self.W_K = nn.Linear(d_model, self.d_k)\n",
    "        self.W_V = nn.Linear(d_model, self.d_k)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        Q = self.W_Q(x)\n",
    "        K = self.W_K(x)\n",
    "        V = self.W_V(x)\n",
    "        \n",
    "        A = torch.matmul(Q, torch.transpose(K, 0, 1)) / torch.sqrt(torch.tensor(self.d_k))\n",
    "        \n",
    "        A = F.softmax(A, dim=1)\n",
    "        \n",
    "        V_prime = torch.matmul(A, V)\n",
    "        \n",
    "        return V_prime\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_head, device):\n",
    "        \n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        self.d_k = int(d_model / 8)\n",
    "        self.n_head = n_head\n",
    "        \n",
    "        self.attentions = []\n",
    "        for i in range(self.n_head):\n",
    "            self.attentions.append(SingleAttention(d_model).to(device))\n",
    "        \n",
    "        self.W_O = nn.Linear(n_head * self.d_k, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        Vs = []\n",
    "        for i in range(self.n_head):\n",
    "            Vs.append(self.attentions[i](x))\n",
    "        \n",
    "        V = torch.cat(tuple(Vs), dim=1)\n",
    "        \n",
    "        x = self.W_O(V)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_head, device):\n",
    "        \n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "        self.mha = MultiHeadAttention(d_model, n_head, device).to(device)\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.mha(x)\n",
    "        x2 = self.ln1(x + x1)\n",
    "        x3 = self.fc(x2)\n",
    "        x4 = self.ln2(x3 + x2)\n",
    "        \n",
    "        return x4\n",
    "\n",
    "class ProtBERT(nn.Module):\n",
    "    def __init__(self, d_model, n_head, vocab_size, device):\n",
    "        \n",
    "        super(ProtBERT, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size+2, embedding_dim=d_model)\n",
    "        self.trans = TransformerBlock(d_model, n_head, device).to(device)\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        #print(x)\n",
    "        #x_embedding = torch.clone(x)\n",
    "        x = self.trans(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d49c14-865b-4626-acd4-03d090f8a7b5",
   "metadata": {},
   "source": [
    "#### Model for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da6c4d22-7405-4fbc-9ebc-c40b3140ea2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        '''in_dim: input layer dim\n",
    "           hidden_dim: hidden layer dim\n",
    "           out_dim: output layer dim'''\n",
    "        \n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        #two fully connected layers\n",
    "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten it first\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        # compute output of fc1, and apply relu activation\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        # compute output layer\n",
    "        # no activation: cross entropy will compute softmax\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b113c11-e840-4970-a389-b7b141314ff4",
   "metadata": {},
   "source": [
    "#### Define Dataset For classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3a38350-d07e-460e-a543-620f5669601f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClsData(torch.utils.data.Dataset):\n",
    "    def __init__(self, families, model, device):\n",
    "        super(ClsData, self).__init__()\n",
    "        \n",
    "        self.data = []\n",
    "        self.label = []\n",
    "        \n",
    "        for family in tqdm(families):\n",
    "            for CLS in family:\n",
    "                self.data.append(CLS)\n",
    "                self.label.append(1)\n",
    "                # randomly choose one negative sample\n",
    "                neg_family = random.choice(families)\n",
    "                neg_CLS = random.choice(neg_family)\n",
    "                self.data.append(neg_CLS)\n",
    "                self.label.append(0)\n",
    "        \n",
    "        self.data = torch.tensor(self.data, dtype=torch.float)\n",
    "        self.label = torch.tensor(self.label, dtype=torch.float)\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.label[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e87755-df51-469f-9370-8585ace560dd",
   "metadata": {},
   "source": [
    "#### Set Up Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fac86d0-ae28-4016-95b0-0c059b119754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ProtBERT(\n",
       "  (embedding): Embedding(27, 256)\n",
       "  (trans): TransformerBlock(\n",
       "    (mha): MultiHeadAttention(\n",
       "      (W_O): Linear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "    (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (fc): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (fc): Linear(in_features=256, out_features=25, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = f'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"using device: {device}\")\n",
    "#filename = 'small_uniprot.txt'\n",
    "filename = 'family_classification_sequences.csv'\n",
    "file = open(filename, \"r\")\n",
    "while(True):\n",
    "    line = file.readline()\n",
    "    if(line != 'sequence' and line[0] != '#'):\n",
    "        break\n",
    "sequences = file.read().rstrip()\n",
    "vocabs = sorted(set(sequences.replace('\\n','')))\n",
    "vocab_to_idx = {vocab: index for index, (vocab) in enumerate(vocabs)}\n",
    "vocab_to_idx['CLS'] = len(vocabs)\n",
    "vocab_to_idx['MASK'] = len(vocabs) + 1\n",
    "\n",
    "d_model = 256\n",
    "n_head = 8\n",
    "vocab_size = len(vocabs)\n",
    "model = ProtBERT(d_model, n_head, vocab_size, device).to(device)\n",
    "\n",
    "checkpoint = torch.load('checkpoint2.pth')\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4ce7ae4-b9ae-4cdb-81ed-d8daa63806d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cls_train(cls_model, cls_optimizer, train_loader, epochs):\n",
    "    cls_model = cls_model.to(device)\n",
    "    cls_model.train()\n",
    "    for epoch in range(1, epochs + 1):    \n",
    "        sum_loss = 0.\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for batch_idx, (data, label) in enumerate(train_loader):\n",
    "            \n",
    "            data, label = data.to(device), label.to(device)\n",
    "            \n",
    "            # zero out prev gradients\n",
    "            cls_optimizer.zero_grad()\n",
    "            \n",
    "            # run the forward pass\n",
    "            output = cls_model(data)\n",
    "            #print(output)\n",
    "            \n",
    "            label = label.reshape((-1,1))\n",
    "            # compute loss/error\n",
    "            loss = F.binary_cross_entropy_with_logits(output, label)\n",
    "            \n",
    "            # compute training accuracy\n",
    "            pred = torch.clone(output)\n",
    "            pred = torch.sigmoid(pred)\n",
    "            pred[pred < float(0.5)] = 0\n",
    "            pred[pred != 0] = 1\n",
    "            \n",
    "            correct += torch.sum((pred == label))\n",
    "            total += len(pred)\n",
    "            #print(classification_report(label.to('cpu').tolist(), pred.to('cpu').tolist()))\n",
    "            \n",
    "            # sum up batch losses\n",
    "            sum_loss += loss.item()\n",
    "\n",
    "            # compute gradients and take a step\n",
    "            loss.backward()\n",
    "            cls_optimizer.step()\n",
    "        # average loss per example\n",
    "        sum_loss /= len(train_loader)\n",
    "        print(f'Epoch: {epoch}, Loss: {sum_loss:.6f}')\n",
    "        \n",
    "        if epoch == epochs:\n",
    "            acc = correct / total\n",
    "            print(f'Accuracy: {acc:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c16481c-7cd4-429b-b783-696d9a12a265",
   "metadata": {},
   "source": [
    "#### Get the Data for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "854e90c9-f65b-43a8-845e-ad6d506961b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 324018/324018 [15:25<00:00, 350.23it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22153\n"
     ]
    }
   ],
   "source": [
    "dataframe = {'Sequences':[]}\n",
    "count = 0\n",
    "length = set()\n",
    "for sequence in tqdm(sequences.split('\\n')):\n",
    "    seq_idx = []\n",
    "    seq_idx.append(vocab_to_idx['CLS'])\n",
    "    for letter in sequence:\n",
    "        seq_idx.append(vocab_to_idx[letter])\n",
    "    \n",
    "    CLS = None\n",
    "    if count == 138252:\n",
    "        length.add(len(seq_idx))\n",
    "        #print(len(seq_idx))\n",
    "        # torch.cuda.empty_cache()\n",
    "        # print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
    "        seq_idx = seq_idx[0:int(len(seq_idx)/2)]\n",
    "        seq_tensor = torch.tensor(seq_idx, dtype=torch.int64).to(device)\n",
    "        embedding = model.embedding(seq_tensor)\n",
    "        CLS = model.trans(embedding)[0].tolist()\n",
    "    else:\n",
    "        seq_tensor = torch.tensor(seq_idx, dtype=torch.int64).to(device)\n",
    "        embedding = model.embedding(seq_tensor)\n",
    "        CLS = model.trans(embedding)[0].tolist()\n",
    "        length.add(len(seq_idx))\n",
    "\n",
    "    dataframe['Sequences'].append(CLS)\n",
    "    count += 1\n",
    "\n",
    "print(max(list(length)))\n",
    "\n",
    "df1 = pd.DataFrame(dataframe)\n",
    "\n",
    "cls_file = 'family_classification_metadata.csv'\n",
    "df2 = pd.read_csv(cls_file)[['Family ID']]\n",
    "\n",
    "df = pd.concat([df1, df2], join = 'outer', axis = 1)\n",
    "grouped_df = df.groupby('Family ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c935a4-ea92-425b-8160-8bf6a41892d4",
   "metadata": {},
   "source": [
    "#### Training Classification (BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2460e1b5-7ea1-4d44-bb51-aead0281b143",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 16493.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 1000 families:\n",
      "Epoch: 1, Loss: 0.693108\n",
      "Epoch: 2, Loss: 0.692583\n",
      "Epoch: 3, Loss: 0.692621\n",
      "Epoch: 4, Loss: 0.692853\n",
      "Epoch: 5, Loss: 0.692764\n",
      "Epoch: 6, Loss: 0.692666\n",
      "Epoch: 7, Loss: 0.692468\n",
      "Epoch: 8, Loss: 0.691601\n",
      "Epoch: 9, Loss: 0.691193\n",
      "Epoch: 10, Loss: 0.691485\n",
      "Accuracy: 0.518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:00<00:00, 24337.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 2000 families:\n",
      "Epoch: 1, Loss: 0.692331\n",
      "Epoch: 2, Loss: 0.692464\n",
      "Epoch: 3, Loss: 0.692096\n",
      "Epoch: 4, Loss: 0.692104\n",
      "Epoch: 5, Loss: 0.691789\n",
      "Epoch: 6, Loss: 0.691936\n",
      "Epoch: 7, Loss: 0.692004\n",
      "Epoch: 8, Loss: 0.691661\n",
      "Epoch: 9, Loss: 0.691482\n",
      "Epoch: 10, Loss: 0.691855\n",
      "Accuracy: 0.509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:00<00:00, 25540.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 3000 families:\n",
      "Epoch: 1, Loss: 0.692652\n",
      "Epoch: 2, Loss: 0.692582\n",
      "Epoch: 3, Loss: 0.692510\n",
      "Epoch: 4, Loss: 0.692473\n",
      "Epoch: 5, Loss: 0.692388\n",
      "Epoch: 6, Loss: 0.692373\n",
      "Epoch: 7, Loss: 0.692510\n",
      "Epoch: 8, Loss: 0.692487\n",
      "Epoch: 9, Loss: 0.692382\n",
      "Epoch: 10, Loss: 0.692488\n",
      "Accuracy: 0.509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:00<00:00, 20433.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 4000 families:\n",
      "Epoch: 1, Loss: 0.691879\n",
      "Epoch: 2, Loss: 0.691753\n",
      "Epoch: 3, Loss: 0.691778\n",
      "Epoch: 4, Loss: 0.691835\n",
      "Epoch: 5, Loss: 0.691792\n",
      "Epoch: 6, Loss: 0.691826\n",
      "Epoch: 7, Loss: 0.691425\n",
      "Epoch: 8, Loss: 0.691288\n",
      "Epoch: 9, Loss: 0.691608\n",
      "Epoch: 10, Loss: 0.691792\n",
      "Accuracy: 0.514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 18286.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5000 families:\n",
      "Epoch: 1, Loss: 0.691512\n",
      "Epoch: 2, Loss: 0.691715\n",
      "Epoch: 3, Loss: 0.691225\n",
      "Epoch: 4, Loss: 0.691139\n",
      "Epoch: 5, Loss: 0.691317\n",
      "Epoch: 6, Loss: 0.692046\n",
      "Epoch: 7, Loss: 0.691579\n",
      "Epoch: 8, Loss: 0.691342\n",
      "Epoch: 9, Loss: 0.691630\n",
      "Epoch: 10, Loss: 0.691851\n",
      "Accuracy: 0.511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6000/6000 [00:00<00:00, 16513.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 6000 families:\n",
      "Epoch: 1, Loss: 0.691607\n",
      "Epoch: 2, Loss: 0.691604\n",
      "Epoch: 3, Loss: 0.691370\n",
      "Epoch: 4, Loss: 0.691804\n",
      "Epoch: 5, Loss: 0.691630\n",
      "Epoch: 6, Loss: 0.691492\n",
      "Epoch: 7, Loss: 0.691683\n",
      "Epoch: 8, Loss: 0.690732\n",
      "Epoch: 9, Loss: 0.691491\n",
      "Epoch: 10, Loss: 0.690384\n",
      "Accuracy: 0.527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7000/7000 [00:00<00:00, 16970.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 7000 families:\n",
      "Epoch: 1, Loss: 0.690639\n",
      "Epoch: 2, Loss: 0.691137\n",
      "Epoch: 3, Loss: 0.691282\n",
      "Epoch: 4, Loss: 0.690734\n",
      "Epoch: 5, Loss: 0.690928\n",
      "Epoch: 6, Loss: 0.690856\n",
      "Epoch: 7, Loss: 0.691585\n",
      "Epoch: 8, Loss: 0.690978\n",
      "Epoch: 9, Loss: 0.691265\n",
      "Epoch: 10, Loss: 0.690791\n",
      "Accuracy: 0.518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7027/7027 [00:00<00:00, 16796.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The entire dataset:\n",
      "Epoch: 1, Loss: 0.692001\n",
      "Epoch: 2, Loss: 0.691962\n",
      "Epoch: 3, Loss: 0.691636\n",
      "Epoch: 4, Loss: 0.691757\n",
      "Epoch: 5, Loss: 0.691654\n",
      "Epoch: 6, Loss: 0.691534\n",
      "Epoch: 7, Loss: 0.691693\n",
      "Epoch: 8, Loss: 0.691790\n",
      "Epoch: 9, Loss: 0.691816\n",
      "Epoch: 10, Loss: 0.691718\n",
      "Accuracy: 0.509\n"
     ]
    }
   ],
   "source": [
    "family = 0\n",
    "families = []\n",
    "cls_model = MLP(d_model, 256, 1)\n",
    "cls_optimizer = optim.SGD(cls_model.parameters(), lr=0.05, momentum=0.9)\n",
    "for key, item in grouped_df:\n",
    "    \n",
    "    if family % 1000 == 0 and family != 0:\n",
    "        cls_dataset = ClsData(families, model, device)\n",
    "        train_loader = data.DataLoader(dataset=cls_dataset, \n",
    "                        batch_size=500,\n",
    "                        shuffle=True)\n",
    "        epochs = 10\n",
    "        print('First {} families:'.format(family))\n",
    "        cls_train(cls_model, cls_optimizer, train_loader, epochs)\n",
    "    \n",
    "    families.append(list(grouped_df.get_group(key)['Sequences']))\n",
    "    family += 1\n",
    "\n",
    "cls_dataset = ClsData(families, model, device)\n",
    "train_loader = data.DataLoader(dataset=cls_dataset, \n",
    "                        batch_size=200,\n",
    "                        shuffle=True)\n",
    "epochs = 10\n",
    "print('The entire dataset:')\n",
    "cls_train(cls_model, cls_optimizer, train_loader, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727beeef-f889-46fd-8333-fad4c0f98c8e",
   "metadata": {},
   "source": [
    "#### WordtoVec classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aba82024-dcc0-425a-882a-b9124c531a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordToVecData(torch.utils.data.Dataset):\n",
    "    def __init__(self, families):\n",
    "        super(WordToVecData, self).__init__()\n",
    "        \n",
    "        self.data = []\n",
    "        self.label = []\n",
    "        \n",
    "        for family in tqdm(families):\n",
    "            for vec in family:\n",
    "                self.data.append(vec)\n",
    "                self.label.append(1)\n",
    "                # randomly choose one negative sample\n",
    "                neg_family = random.choice(families)\n",
    "                neg_vec = random.choice(neg_family)\n",
    "                self.data.append(neg_vec)\n",
    "                self.label.append(0)\n",
    "        \n",
    "        self.data = torch.tensor(self.data, dtype=torch.float)\n",
    "        self.label = torch.tensor(self.label, dtype=torch.float)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.label[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98274478-f840-426a-b591-e6d02f543aff",
   "metadata": {},
   "source": [
    "#### Training Classification (Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00e2aa2a-26ef-40a0-8c34-0f904bc87fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 16598.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.918050\n",
      "Epoch: 2, Loss: 0.635993\n",
      "Epoch: 3, Loss: 0.624709\n",
      "Epoch: 4, Loss: 0.626691\n",
      "Epoch: 5, Loss: 0.623777\n",
      "Epoch: 6, Loss: 0.629583\n",
      "Epoch: 7, Loss: 0.621729\n",
      "Epoch: 8, Loss: 0.628340\n",
      "Epoch: 9, Loss: 0.656839\n",
      "Epoch: 10, Loss: 0.683181\n",
      "Accuracy: 0.548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:00<00:00, 23280.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.691830\n",
      "Epoch: 2, Loss: 0.648510\n",
      "Epoch: 3, Loss: 0.641624\n",
      "Epoch: 4, Loss: 0.644351\n",
      "Epoch: 5, Loss: 0.632373\n",
      "Epoch: 6, Loss: 0.638298\n",
      "Epoch: 7, Loss: 0.661344\n",
      "Epoch: 8, Loss: 0.643872\n",
      "Epoch: 9, Loss: 0.639670\n",
      "Epoch: 10, Loss: 0.636381\n",
      "Accuracy: 0.644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:00<00:00, 24932.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.650589\n",
      "Epoch: 2, Loss: 0.641074\n",
      "Epoch: 3, Loss: 0.642069\n",
      "Epoch: 4, Loss: 0.641184\n",
      "Epoch: 5, Loss: 0.642199\n",
      "Epoch: 6, Loss: 0.639744\n",
      "Epoch: 7, Loss: 0.639338\n",
      "Epoch: 8, Loss: 0.639199\n",
      "Epoch: 9, Loss: 0.647120\n",
      "Epoch: 10, Loss: 0.638023\n",
      "Accuracy: 0.635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:00<00:00, 20221.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.645117\n",
      "Epoch: 2, Loss: 0.641550\n",
      "Epoch: 3, Loss: 0.639623\n",
      "Epoch: 4, Loss: 0.636080\n",
      "Epoch: 5, Loss: 0.638415\n",
      "Epoch: 6, Loss: 0.640109\n",
      "Epoch: 7, Loss: 0.665185\n",
      "Epoch: 8, Loss: 0.638915\n",
      "Epoch: 9, Loss: 0.637107\n",
      "Epoch: 10, Loss: 0.665449\n",
      "Accuracy: 0.579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 18368.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.680393\n",
      "Epoch: 2, Loss: 0.667234\n",
      "Epoch: 3, Loss: 0.686472\n",
      "Epoch: 4, Loss: 0.691886\n",
      "Epoch: 5, Loss: 0.690515\n",
      "Epoch: 6, Loss: 0.689566\n",
      "Epoch: 7, Loss: 0.688043\n",
      "Epoch: 8, Loss: 0.688127\n",
      "Epoch: 9, Loss: 0.688282\n",
      "Epoch: 10, Loss: 0.685242\n",
      "Accuracy: 0.541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6000/6000 [00:00<00:00, 16406.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.671846\n",
      "Epoch: 2, Loss: 0.648210\n",
      "Epoch: 3, Loss: 0.652663\n",
      "Epoch: 4, Loss: 0.641747\n",
      "Epoch: 5, Loss: 0.642654\n",
      "Epoch: 6, Loss: 0.648403\n",
      "Epoch: 7, Loss: 0.646867\n",
      "Epoch: 8, Loss: 0.647661\n",
      "Epoch: 9, Loss: 0.645399\n",
      "Epoch: 10, Loss: 0.640117\n",
      "Accuracy: 0.642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7000/7000 [00:00<00:00, 16722.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.637734\n",
      "Epoch: 2, Loss: 0.633219\n",
      "Epoch: 3, Loss: 0.643028\n",
      "Epoch: 4, Loss: 0.639594\n",
      "Epoch: 5, Loss: 0.642396\n",
      "Epoch: 6, Loss: 0.642078\n",
      "Epoch: 7, Loss: 0.641197\n",
      "Epoch: 8, Loss: 0.649333\n",
      "Epoch: 9, Loss: 0.641734\n",
      "Epoch: 10, Loss: 0.646760\n",
      "Accuracy: 0.626\n"
     ]
    }
   ],
   "source": [
    "filename = 'family_classification_protVec.csv'\n",
    "dataframe = {'Sequences':[]}\n",
    "with open(filename, \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.strip()\n",
    "        line = line.split(',')\n",
    "        #print(line)\n",
    "        line = list(map(float, line))\n",
    "        dataframe['Sequences'].append(line)\n",
    "\n",
    "df1 = pd.DataFrame(dataframe)\n",
    "df = pd.concat([df1, df2], join = 'outer', axis = 1)\n",
    "grouped_df = df.groupby('Family ID')\n",
    "\n",
    "family = 0\n",
    "families = []\n",
    "cls_model = MLP(100, 256, 1)\n",
    "cls_optimizer = optim.SGD(cls_model.parameters(), lr=0.05, momentum=0.9)\n",
    "\n",
    "for key, item in grouped_df:\n",
    "    \n",
    "    if family % 1000 == 0 and family != 0:\n",
    "        cls_dataset = WordToVecData(families)\n",
    "        train_loader = data.DataLoader(dataset=cls_dataset, \n",
    "                        batch_size=500,\n",
    "                        shuffle=True)\n",
    "        epochs = 10\n",
    "        cls_train(cls_model, cls_optimizer, train_loader, epochs)\n",
    "    \n",
    "    families.append(list(grouped_df.get_group(key)['Sequences']))\n",
    "    family += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252495a2-4300-4c38-a0ba-294c91ea333d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
