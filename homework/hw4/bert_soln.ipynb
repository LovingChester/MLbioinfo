{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f4f2022-fce8-4abe-bbd1-fbeeca0ee8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "import torch.optim as optim\n",
    "import einops\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cdd459c-08cf-4a60-b1b7-02764bcb2b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequences(data.Dataset):\n",
    "    '''This class reads in the sequences, extract the alphabet'''\n",
    "\n",
    "    def __init__(self, filename, mask_prob):\n",
    "        super(Sequences, self).__init__()\n",
    "        '''read protein sequences from file'''\n",
    "        self.mask_prob = mask_prob  # masking probability\n",
    "        self.sequences = []  # set of sequences\n",
    "        self.alphabet = {}  # set of characters/symbols\n",
    "        with open(filename, \"r\") as f:\n",
    "            for line in f.readlines():\n",
    "                # skip lines beginning with # or \"sequence\"\n",
    "                a = line.strip()\n",
    "                if a[0] == \"#\":\n",
    "                    continue\n",
    "                elif a == \"sequence\":\n",
    "                    continue\n",
    "                else:\n",
    "                    self.sequences.append(a)\n",
    "                    for c in a:\n",
    "                        self.alphabet[c] = True\n",
    "\n",
    "        # find the max seq len and set the block size for transformer\n",
    "        self.block_size = max([len(s) for s in self.sequences])\n",
    "        self.block_size += 1  # first token is always CLS\n",
    "        print(\"BLOCK SIZE\", self.block_size)\n",
    "\n",
    "        # distinct chars/AA\n",
    "        self.alphabet = sorted(self.alphabet.keys())\n",
    "        self.alphabet_idx = {aa: i for i, aa in enumerate(self.alphabet)}\n",
    "\n",
    "        # add special non-alphabet tokens\n",
    "        self.a_sz = len(self.alphabet) # orig alphabet size, without extra tokens\n",
    "        self.alphabet_idx['MASK'] = self.a_sz\n",
    "        self.alphabet_idx['CLS'] = self.a_sz + 1\n",
    "        self.alphabet_idx['PAD'] = self.a_sz + 2\n",
    "\n",
    "    def __len__(self):\n",
    "        '''return number of sequences'''\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''tokenize the sequence at idx -- one token per AA\n",
    "        make sure all sequences are padded to block_size\n",
    "        add CLS to front and PAD tokens if sequence is short\n",
    "        replace masked positions with MASK\n",
    "        return tokenized sequence, mask array, and true labels/tokens'''\n",
    "        S = self.sequences[idx]\n",
    "        # add CLS to front\n",
    "        tokenized_seq = [self.alphabet_idx['CLS']]\n",
    "        # actual AA sequence\n",
    "        tokenized_seq.extend([self.alphabet_idx[S[i]] for i in range(len(S))])\n",
    "        # PAD as remaining elements\n",
    "        pad_len = self.block_size - len(tokenized_seq)\n",
    "        tokenized_seq.extend([self.alphabet_idx['PAD'] for i in range(pad_len)])\n",
    "\n",
    "        tokenized_seq = np.array(tokenized_seq)\n",
    "        labels = tokenized_seq.copy()  # labels are same as original tokens\n",
    "\n",
    "        # MASK out random positions given by mask_prob\n",
    "        # notice that PAD positions are never masked\n",
    "        num_masked = int(len(S) * self.mask_prob)\n",
    "        mask_idxs = np.random.choice(len(S), num_masked, replace=False)\n",
    "        mask_idxs += 1  # offset by 1 since CLS is 1st token\n",
    "        \n",
    "        # create mask array for masked positions\n",
    "        mask_ary = np.zeros(self.block_size, dtype=int)\n",
    "        mask_ary[mask_idxs] = 1        \n",
    "        \n",
    "        # 10% of the mask_idxs will be left as is, without replacing with MASK\n",
    "        # or random token. So select the 90% of remaining idxs\n",
    "        mask_idxs = np.random.choice(mask_idxs, int(len(mask_idxs)*0.9), replace=False)\n",
    "        \n",
    "        # replace remaining mask_idxs with MASK token\n",
    "        tokenized_seq[mask_idxs] = self.alphabet_idx['MASK']\n",
    "        \n",
    "        # select 10% of mask idxs to replace with random token\n",
    "        rand_idxs = np.random.choice(mask_idxs, int(len(mask_idxs)*0.1), replace=False)        \n",
    "        # ideally replace proportional to token frequency, but we'll do it at random\n",
    "        rand_tokens = np.random.choice(self.a_sz, len(rand_idxs), replace=False)\n",
    "        tokenized_seq[rand_idxs] = rand_tokens\n",
    "\n",
    "        return tokenized_seq, mask_ary, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bb09723-dcac-4870-9da9-9ceda2138ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    '''Self Attention'''\n",
    "\n",
    "    def __init__(self, d, dk):\n",
    "        '''define WQ, WK, WV projection matrices:\n",
    "        d: d_model is the original model dimension\n",
    "        dk: projection dimension for query, keys and values\n",
    "        '''\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.d = d  # d_model\n",
    "        self.dk = dk  # d_k: projection dimension\n",
    "        self.WQ = nn.Linear(self.d, self.dk, bias=False)\n",
    "        self.WK = nn.Linear(self.d, self.dk, bias=False)\n",
    "        self.WV = nn.Linear(self.d, self.dk, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''project the context onto key, query and value spaces and\n",
    "        return the final value vectors\n",
    "        '''\n",
    "        # input shape: (batch_size, block_size, d)\n",
    "        # let batch_size=b, block_size=l, num_heads=h\n",
    "        Q = self.WQ(x)  # shape: b, l, dk\n",
    "        K = self.WK(x)  # shape: b, l, dk\n",
    "        V = self.WV(x)  # shape: b, l, dk\n",
    "\n",
    "        K = torch.transpose(K, 1, 2)  # K.T transpose\n",
    "        QKT = torch.bmm(Q, K)  # shape: b, l, l\n",
    "\n",
    "        # attention matrix\n",
    "        # row specifies weights for the value vectors, row add up to one\n",
    "        A = F.softmax(QKT / np.sqrt(self.dk), dim=2)  # shape: b, l, l\n",
    "\n",
    "        V = torch.bmm(A, V)  # shape: b, l, dk\n",
    "        return V\n",
    "\n",
    "\n",
    "class SepHeads_SelfAttention(nn.Module):\n",
    "    '''Separate Headed Self Attention: List of Attention Heads\n",
    "    This is a straightforward implementation of the multiple heads.\n",
    "    We have separate WQ, WK and WV matrices, one per head.'''\n",
    "\n",
    "    def __init__(self, d, dk, num_heads):\n",
    "        '''create separate heads:\n",
    "        d: d_model dimension\n",
    "        dk: projection dimension for query, keys and values\n",
    "        num_heads: number of attention heads\n",
    "        '''\n",
    "        super(SepHeads_SelfAttention, self).__init__()\n",
    "        self.d = d  # d_model\n",
    "        self.dk = dk  # d_k: projection dimension\n",
    "        self.num_heads = num_heads  # number of attention heads\n",
    "\n",
    "        self.sa_layers = nn.ModuleList()\n",
    "        for i in range(self.num_heads):\n",
    "            self.sa_layers.append(SelfAttention(self.d, self.dk))\n",
    "\n",
    "        self.WO = nn.Linear(self.dk * self.num_heads, self.d, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''use separate attention heads, and concat values'''\n",
    "        # input shape: (batch_size, block_size, d)\n",
    "        # let batch_size=b, block_size=l, num_heads=h\n",
    "        V = []\n",
    "        for i in range(self.num_heads):\n",
    "            V.append(self.sa_layers[i](x))\n",
    "\n",
    "        # concat all the value vectors from the heads\n",
    "        V = torch.cat(V, dim=2)  # shape: b, l, h x dk\n",
    "        # project back to d_model\n",
    "        x = self.WO(V)  # shape: b, l, d\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiHead_SelfAttention(nn.Module):\n",
    "    '''Multi Headed Self Attention:\n",
    "    Instead of using a list of attention heads with separate WQ, WK, WV matrices,\n",
    "    we combine all heads into one, and use a single WQ, WK and WV matrix.\n",
    "    Each matrix maps the d-dim input block into h*dk dim space, where h is num_heads.\n",
    "    We have to carefully keep the heads separate for softmax to achieve the same\n",
    "    effect at from the list of heads. We do that via einops and the very useful\n",
    "    torch.einsum function.\n",
    "    \n",
    "    This function is much more efficient than using separate heads.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, d, dk, num_heads):\n",
    "        '''create multi-heads -- joint heads:\n",
    "        d: d_model dimension\n",
    "        dk: projection dimension for query, keys and values\n",
    "        num_heads: number of attention heads\n",
    "        '''\n",
    "        super(MultiHead_SelfAttention, self).__init__()\n",
    "        self.d = d  # d_model\n",
    "        self.dk = dk  # d_k: projection dimension\n",
    "        self.num_heads = num_heads  # number of attention heads\n",
    "\n",
    "        self.WQ = nn.Linear(self.d, self.dk * self.num_heads, bias=False)\n",
    "        self.WK = nn.Linear(self.d, self.dk * self.num_heads, bias=False)\n",
    "        self.WV = nn.Linear(self.d, self.dk * self.num_heads, bias=False)\n",
    "        self.WO = nn.Linear(self.dk * self.num_heads, self.d, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input shape: (batch_size, block_size, d)\n",
    "        # let batch_size=b, block_size=l, num_heads=h, d_model=d\n",
    "        Q = self.WQ(x)  # size: (b, l, h*dk)\n",
    "        K = self.WK(x)  # size: (b, l, h*dk)\n",
    "        V = self.WV(x)  # size: (b, l, h*dk)\n",
    "\n",
    "        # split Q, K, V into heads and dk, move heads up front; KT is transpose of K\n",
    "        Q = einops.rearrange(\n",
    "            Q, 'b l (h dk)-> b h l dk', h=self.num_heads\n",
    "        )  # size: (b, h, l, dk)\n",
    "        KT = einops.rearrange(\n",
    "            K, 'b l (h dk)-> b h dk l', h=self.num_heads\n",
    "        )  # size: (b, h, dk, l)\n",
    "        V = einops.rearrange(\n",
    "            V, 'b l (h dk)-> b h l dk', h=self.num_heads\n",
    "        )  # size: (b, h, l, dk)\n",
    "\n",
    "        # compute Q x K.T, output is (b, h, l, l)\n",
    "        QKT = torch.einsum('bhik,bhkj->bhij', Q, KT)\n",
    "        A = F.softmax(QKT / np.sqrt(self.dk), dim=3)  # softmax along last dim\n",
    "\n",
    "        # new value representation\n",
    "        V = torch.einsum('bhik,bhkj->bhij', A, V)  # size: (b, h, l, dk)\n",
    "        V = einops.rearrange(V, 'b h l dk -> b l (h dk)')  # size: (b, l, h*dk)\n",
    "\n",
    "        # shape: b, l, h x dk\n",
    "        x = self.WO(V)  # shape: b, l, d\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    '''Transformer Block: multi-head or separate heads of attention,\n",
    "    followed by layernorm, ffn, and another layernorm\n",
    "    '''\n",
    "\n",
    "    def __init__(self, d, dk, num_heads, block_size, use_sepheads):\n",
    "        '''\n",
    "        d: d_model dimension\n",
    "        dk: projection dimension\n",
    "        num_heads: number of attention heads\n",
    "        use_sepheads: use separate heads or multiheads,\n",
    "                      multiheads is much more efficient\n",
    "        '''\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.use_sepheads = use_sepheads\n",
    "        self.drop_prob = 0.1\n",
    "\n",
    "        if self.use_sepheads:\n",
    "            # uses for loop for separate heads\n",
    "            self.mhsa = SepHeads_SelfAttention(d, dk, num_heads)\n",
    "        else:\n",
    "            # this is more efficient\n",
    "            self.mhsa = MultiHead_SelfAttention(d, dk, num_heads)\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(d)  # layer norm\n",
    "        self.ffn = nn.Sequential( #FFN module\n",
    "            nn.Linear(d, d),  # linear layer\n",
    "            nn.ReLU(),  # relu\n",
    "            nn.Linear(d, d)  # linear layer\n",
    "        )\n",
    "        self.ln2 = nn.LayerNorm(d)  # layer norm\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input shape: (batch_size, block_size, d)\n",
    "        # let batch_size=b, block_size=l, num_heads=h, d_model=d\n",
    "        x_sa = self.mhsa(x)  # multiple attention heads\n",
    "        x_sa = F.dropout(x_sa, p=self.drop_prob)\n",
    "        x_ln1 = self.ln1(x + x_sa)  # residual layer + layer norm\n",
    "        # two linear layers with relu in between\n",
    "        x_ffn = self.ffn(x_ln1)\n",
    "        x_ffn = F.dropout(x_ffn, p=self.drop_prob)\n",
    "        x_ln2 = self.ln2(x_ln1 + x_ffn)  # residual layer + layer norm\n",
    "        return x_ln2\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    '''Transformer model:\n",
    "    input is a block of tokens: first token is always CLS\n",
    "    MASK token for positions for training the masked language model\n",
    "    PAD tokens at end for sequences shorter than block size'''\n",
    "\n",
    "    def __init__(\n",
    "        self, d, dk, block_size, num_layers, num_heads, alphabet_idx, use_sepheads\n",
    "    ):\n",
    "        '''\n",
    "        d: d_model dimension\n",
    "        dk: projection dimension\n",
    "        block_size: the max sequence length\n",
    "        num_layers: how many transformer blocks/layers?\n",
    "        num_heads: number of attention heads\n",
    "        alphabet_idx: dict of tokens to token ids (ints)\n",
    "        use_sepheads: use separate heads or joint heads (multiheads),\n",
    "                      multiheads is much more efficient\n",
    "\n",
    "        '''\n",
    "        super(Transformer, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.drop_prob = 0.1  # for dropout layer\n",
    "\n",
    "        # embedding layer to map tokens to d dim vectors\n",
    "        self.embed = nn.Embedding(len(alphabet_idx), d, padding_idx=alphabet_idx['PAD'])\n",
    "\n",
    "        # learnable position embeddings, one per sequence element in block\n",
    "        # can also use sine/cosine embeddings: not done here!\n",
    "        self.pos_embed = nn.Embedding(block_size, d)\n",
    "\n",
    "        # list of transformer blocks/layers\n",
    "        tb_list = [\n",
    "            TransformerBlock(d, dk, num_heads, block_size, use_sepheads)\n",
    "            for i in range(self.num_layers)\n",
    "        ]\n",
    "        # combine all layers into one \"sequential\" layer\n",
    "        self.layers = nn.Sequential(*tb_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input shape: batch_size (b), block_size (l)\n",
    "        # d is d_model\n",
    "        p = self.pos_embed.weight  # shape: l, d\n",
    "        x = self.embed(x) + p  # add pos embeddings, shape: b, l, d\n",
    "        x = F.dropout(x, p=self.drop_prob)  # dropout\n",
    "        x = self.layers(x)  # shape: (b, l, d)\n",
    "        return x\n",
    "\n",
    "\n",
    "class BERT(nn.Module):\n",
    "    '''BERT model'''\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d,  # d_model\n",
    "        dk,  # projection dimension for queries, keys and values\n",
    "        block_size,  # max sequence length\n",
    "        num_layers,  # number of transformer layers\n",
    "        num_heads,  # number of attention heads\n",
    "        alphabet_idx,  # mapping from alphabet/token to id\n",
    "        alphabet,  # alphabet/tokens (all AA + PAD + MASK + CLS)\n",
    "        use_sepheads,  # use separate or joint attention heads\n",
    "    ):\n",
    "        super(BERT, self).__init__()\n",
    "        self.d = d\n",
    "        self.dk = dk\n",
    "        self.block_size = block_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.use_sepheads = use_sepheads\n",
    "        self.alphabet_idx = alphabet_idx\n",
    "\n",
    "        # transformer model\n",
    "        self.transformer = Transformer(\n",
    "            d, dk, block_size, num_layers, num_heads, alphabet_idx, use_sepheads\n",
    "        )\n",
    "\n",
    "        # map transformer model to one of the tokens (for classification)\n",
    "        self.linear = nn.Linear(d, len(alphabet))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input shape: batch_size (b), block_size (l)\n",
    "        x = self.transformer(x)  # shape: b, l, d\n",
    "        x = self.linear(x)  # shape: b, l, #tokens\n",
    "        return x\n",
    "\n",
    "    def save_transformer(self, args):\n",
    "        '''save the transformer portion'''\n",
    "        fname = f'transformer_d{self.d}_dk{self.dk}_l{self.num_layers}'\n",
    "        fname += f'_h{self.num_heads}_lr{args.learning_rate}'\n",
    "        fname += f'_e{args.epochs}_j{args.jobid}.pth'\n",
    "        saveinfo = {\n",
    "            'd': self.d,\n",
    "            'dk': self.dk,\n",
    "            'l': self.num_layers,\n",
    "            'h': self.num_heads,\n",
    "            'sh': self.use_sepheads,\n",
    "            'block_size': self.block_size,\n",
    "            'alphabet_idx': self.alphabet_idx,\n",
    "            'model': self.transformer.state_dict(),\n",
    "        }\n",
    "        torch.save(saveinfo, fname)\n",
    "\n",
    "    def checkpoint(self, args, bidx, e, running_loss, optimizer):\n",
    "        '''check point the model and optimizer states\n",
    "        useful to resume training later'''\n",
    "        ckpt_fname = f'ckpt_J{args.jobid}.pth'\n",
    "        checkpoint = {\n",
    "            'd': self.d,\n",
    "            'dk': self.dk,\n",
    "            'l': self.num_layers,\n",
    "            'h': self.num_heads,\n",
    "            'alphabex_idx': self.alphabet_idx,\n",
    "            'batch': bidx,\n",
    "            'epoch': e,\n",
    "            'loss': running_loss,\n",
    "            'state_dict': self.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }\n",
    "        torch.save(checkpoint, ckpt_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52c89c15-0f42-45e3-b53e-23c07fd139ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='bert.py')\n",
    "    parser.add_argument('-f', dest='fname')\n",
    "    parser.add_argument('-d', default=256, type=int) #d_model\n",
    "    parser.add_argument('-dk', default=32, type=int) #d_k\n",
    "    parser.add_argument('-l', dest = 'num_layers', default=1, type=int)\n",
    "    parser.add_argument('-H', dest = 'num_heads', default=8, type=int)\n",
    "    parser.add_argument('-sH', dest = 'use_sepheads', default = False, \n",
    "                                action='store_true') # use separate heads?\n",
    "    parser.add_argument('-m', dest='mask_prob', default=0.15, type=float)    \n",
    "    parser.add_argument('-e', dest='epochs', default=10, type=int)\n",
    "    parser.add_argument('-nw', dest='num_workers', default=0, type=int)\n",
    "    parser.add_argument('-b', dest='batch_size', default=4, type=int)\n",
    "    parser.add_argument('-lr', dest='learning_rate', default=0.01, type=float)\n",
    "    parser.add_argument('-wd', dest='weight_decay', default=0, type=float)\n",
    "    parser.add_argument('-j', dest='jobid', default=1, type=int)\n",
    "    parser.add_argument('-D', dest='device', default=0, type=int)\n",
    "    parser.add_argument('-c', dest='chkpt_fname', default=None) # if given, resume from checkpoint\n",
    "    \n",
    "    # set the input args for running the code\n",
    "    cmd = \"-f small_uniprot.txt \"\n",
    "    cmd += \"-lr 1e-4 -wd 1e-7 -e 10\"\n",
    "    #cmd += \"-c ckpt_J1.pth\"\n",
    "    args = parser.parse_args(cmd.split())\n",
    "    \n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae061c82-27c4-4d85-b1e8-1cbfc71755ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(fname='small_uniprot.txt', d=256, dk=32, num_layers=1, num_heads=8, use_sepheads=False, mask_prob=0.15, epochs=10, num_workers=0, batch_size=4, learning_rate=0.0001, weight_decay=1e-07, jobid=1, device=0, chkpt_fname=None)\n",
      "using device NVIDIA GeForce RTX 3090\n",
      "BLOCK SIZE 990\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BERT(\n",
       "  (transformer): Transformer(\n",
       "    (embed): Embedding(27, 256, padding_idx=26)\n",
       "    (pos_embed): Embedding(990, 256)\n",
       "    (layers): Sequential(\n",
       "      (0): TransformerBlock(\n",
       "        (mhsa): MultiHead_SelfAttention(\n",
       "          (WQ): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (WK): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (WV): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (WO): Linear(in_features=256, out_features=256, bias=False)\n",
       "        )\n",
       "        (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=256, out_features=24, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Main training wrapper code\n",
    "args = parse_args()\n",
    "print(args)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = f\"cuda:{args.device}\"\n",
    "    print(\"using device\", torch.cuda.get_device_name(device))\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    \n",
    "# read sequences, create dataset\n",
    "S = Sequences(args.fname, args.mask_prob)\n",
    "data_gen = data.DataLoader(S, \n",
    "                           batch_size=args.batch_size, \n",
    "                           num_workers=args.num_workers, shuffle=True)\n",
    "\n",
    "# create the BERT model\n",
    "model = BERT(args.d, args.dk, S.block_size, args.num_layers, \n",
    "                    args.num_heads, S.alphabet_idx, S.alphabet, args.use_sepheads)\n",
    "\n",
    "# use suggested transformer betas\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.learning_rate, \n",
    "                       betas = (0.9, 0.98),\n",
    "                       weight_decay=args.weight_decay)\n",
    "\n",
    "prev_e = 0 # epoch number\n",
    "if args.chkpt_fname is not None:\n",
    "    saveinfo = torch.load(args.chkpt_fname)\n",
    "    prev_e = saveinfo['epoch']+1\n",
    "    model.load_state_dict(saveinfo['state_dict'])\n",
    "    optimizer.load_state_dict(saveinfo['optimizer'])\n",
    "    print(\"resume from epoch:\", prev_e) # resume from prev_e\n",
    "\n",
    "model = model.to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0b143c3-f1e1-45a8-99d9-dd0a351882ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 250/250 [00:02<00:00, 98.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 143421.14875793457 249 573.6845950317382 0.0893734847180085 6746.0 75481.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 250/250 [00:02<00:00, 107.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 134677.72203063965 249 538.7108881225586 0.12095391211146839 9028.0 74640.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 250/250 [00:02<00:00, 107.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 133828.6978149414 249 535.3147912597656 0.12414963713395971 9289.0 74821.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 250/250 [00:02<00:00, 106.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 133506.07514953613 249 534.0243005981446 0.12491376107838455 9415.0 75372.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 250/250 [00:02<00:00, 106.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 133231.82731628418 249 532.9273092651367 0.12503824307966532 9400.0 75177.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 250/250 [00:02<00:00, 108.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 133032.2057647705 249 532.128823059082 0.12536586011892936 9466.0 75507.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 250/250 [00:02<00:00, 107.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 132769.43391418457 249 531.0777356567382 0.12696506258841114 9514.0 74934.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 250/250 [00:02<00:00, 107.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 132619.56163024902 249 530.4782465209961 0.12879863110002784 9710.0 75389.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 250/250 [00:02<00:00, 105.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 132590.02340698242 249 530.3600936279297 0.12933135530359224 9764.0 75496.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 250/250 [00:02<00:00, 107.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 132367.76065063477 249 529.4710426025391 0.13242138875358553 9787.0 73908.0\n",
      "finished in time 23.862635374069214 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# usual boilerplate training loop over epochs\n",
    "start_t = time.time()\n",
    "for e in range(prev_e, prev_e + args.epochs):\n",
    "    running_loss = 0.\n",
    "    correct = 0.\n",
    "    num_masked = 0.\n",
    "    for bidx, (block, mask, labels) in enumerate(tqdm(data_gen)):\n",
    "        block = block.to(device)\n",
    "        mask = mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        # shape: b, l, C\n",
    "        # where b is batch_size, l is block_size, C is number of classes\n",
    "        preds = model(block) \n",
    "        \n",
    "        #cross_entropy expects b, C, l\n",
    "        preds = preds.swapaxes(1,2) # shape: b, C, l\n",
    "        \n",
    "        # retain loss per position, since we will zero out non-mask positions\n",
    "        loss = F.cross_entropy(preds, labels, reduction='none')\n",
    "        loss = torch.sum(loss*mask)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # compute number of correct predictions, keep track of num_masked for acc\n",
    "        pred_labels = torch.argmax(preds, dim=1)\n",
    "        correct += torch.sum((pred_labels == labels)*mask).item()\n",
    "        num_masked += torch.sum(torch.where(mask == 1)[0]).item()\n",
    "        \n",
    "        # checkpoint every 100 batches\n",
    "        if bidx % 100 == 0:\n",
    "            model.checkpoint(args, bidx, e, running_loss, optimizer)\n",
    "\n",
    "    # print loss at end of each epoch\n",
    "    acc = correct/num_masked\n",
    "    print(\"epoch\", e, running_loss, bidx, running_loss / (bidx + 1), acc, correct, num_masked)\n",
    "    model.checkpoint(args, bidx, e, running_loss, optimizer)\n",
    "\n",
    "# save the transformer model for downstream classification\n",
    "model.save_transformer(args)\n",
    "\n",
    "end_t = time.time()\n",
    "print(\"finished in time\", end_t - start_t, args.num_workers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
