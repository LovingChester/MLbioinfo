{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7162a88d-2d2e-4775-8798-adcd03c91673",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e426e686-c851-4a79-a24b-7589fa1a0fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89fa151-78e5-4576-a6bb-44cf4516226b",
   "metadata": {},
   "source": [
    "#### Single Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d32d1a08-ff7a-4b26-a0cd-3e1987893678",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleAttention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        \n",
    "        super(SingleAttention, self).__init__()\n",
    "        \n",
    "        self.d_k = int(d_model / 8)\n",
    "        \n",
    "        self.W_Q = nn.Linear(d_model, self.d_k)\n",
    "        self.W_K = nn.Linear(d_model, self.d_k)\n",
    "        self.W_V = nn.Linear(d_model, self.d_k)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        Q = self.W_Q(x)\n",
    "        K = self.W_K(x)\n",
    "        V = self.W_V(x)\n",
    "        \n",
    "        A = torch.matmul(Q, torch.transpose(K, 0, 1)) / torch.sqrt(torch.tensor(self.d_k))\n",
    "        \n",
    "        A = F.softmax(A, dim=1)\n",
    "        \n",
    "        V_prime = torch.matmul(A, V)\n",
    "        \n",
    "        return V_prime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0159f60f-6f08-4d7a-bb0f-3e75a84d9c65",
   "metadata": {},
   "source": [
    "#### Multi-head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bca2c13-a939-4572-ab8e-f2d1e10c1cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_head, device):\n",
    "        \n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        self.d_k = int(d_model / 8)\n",
    "        self.n_head = n_head\n",
    "        \n",
    "        self.attentions = []\n",
    "        for i in range(self.n_head):\n",
    "            self.attentions.append(SingleAttention(d_model).to(device))\n",
    "        \n",
    "        self.W_O = nn.Linear(n_head * self.d_k, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        Vs = []\n",
    "        for i in range(self.n_head):\n",
    "            Vs.append(self.attentions[i](x))\n",
    "        \n",
    "        V = torch.cat(tuple(Vs), dim=1)\n",
    "        \n",
    "        x = self.W_O(V)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e09348a-fa4c-4a91-8dde-13adbb1bdc22",
   "metadata": {},
   "source": [
    "#### Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dd4e6e3-cfd8-4fbc-8591-5cd9723bae4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_head, device):\n",
    "        \n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "        self.mha = MultiHeadAttention(d_model, n_head, device).to(device)\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.mha(x)\n",
    "        x2 = self.ln1(x + x1)\n",
    "        x3 = self.fc(x2)\n",
    "        x4 = self.ln2(x3 + x2)\n",
    "        \n",
    "        return x4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40f9273-6e3d-446b-8eee-c49f3bcc569d",
   "metadata": {},
   "source": [
    "#### BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "410d7daa-4026-4574-81e6-b51863b6619f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProtBERT(nn.Module):\n",
    "    def __init__(self, d_model, n_head, vocab_size, device):\n",
    "        \n",
    "        super(ProtBERT, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size+2, embedding_dim=d_model)\n",
    "        self.trans = TransformerBlock(d_model, n_head, device).to(device)\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        #print(x)\n",
    "        #x_embedding = torch.clone(x)\n",
    "        x = self.trans(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf0b2952-b47e-4b11-a186-4bb942e74abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tb = TransformerBlock(128, 8)\n",
    "\n",
    "# x = torch.tensor([1,2,3])\n",
    "# ex = nn.Embedding(4, 128)\n",
    "# #print(ex(x))\n",
    "# #tb(ex(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd82109-b3f4-40ce-bab3-11b954fd7c1d",
   "metadata": {},
   "source": [
    "#### Define Dataset (Map Style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66f20e1c-5a59-4f1a-8dea-6f30b867ffd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequences(torch.utils.data.Dataset):\n",
    "    def __init__(self, filename):\n",
    "        \n",
    "        self.seq_idxes = []  # set of sequence indexs\n",
    "        \n",
    "        file = open(filename, \"r\")\n",
    "        \n",
    "        while(True):\n",
    "            line = file.readline()\n",
    "            if(line != 'sequence' and line[0] != '#'):\n",
    "                break\n",
    "        \n",
    "        sequences = file.read().rstrip()\n",
    "        self.vocabs = sorted(set(sequences.replace('\\n','')))\n",
    "        self.vocab_to_idx = {vocab: index for index, (vocab) in enumerate(self.vocabs)}\n",
    "        self.vocab_to_idx['CLS'] = len(self.vocabs)\n",
    "        self.vocab_to_idx['MASK'] = len(self.vocabs) + 1\n",
    "        \n",
    "        for sequence in tqdm(sequences.split('\\n')):\n",
    "            seq_idx = []\n",
    "            seq_idx.append(self.vocab_to_idx['CLS'])\n",
    "            for letter in sequence:\n",
    "                seq_idx.append(self.vocab_to_idx[letter])\n",
    "            self.seq_idxes.append(torch.tensor(seq_idx, dtype=torch.int64))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.seq_idxes)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.seq_idxes[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131d7150-27ec-4c0b-8668-e6ce378fd53c",
   "metadata": {},
   "source": [
    "#### Iterable Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c102b93-62de-4bd2-a248-c7671c95faac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class IterableSequences(torch.utils.data.IterableDataset):\n",
    "#     def __init__(self, filename, num_workers):\n",
    "        \n",
    "#         super(IterableSequences, self).__init__()\n",
    "#         self.seq_idxes = []  # set of sequence indexs\n",
    "#         self.num_workers = num_workers\n",
    "        \n",
    "#         file = open(filename, \"r\")\n",
    "        \n",
    "#         while(True):\n",
    "#             line = file.readline()\n",
    "#             if(line != 'sequence' and line[0] != '#'):\n",
    "#                 break\n",
    "        \n",
    "#         sequences = file.read().rstrip()\n",
    "#         self.vocabs = sorted(set(sequences.replace('\\n','')))\n",
    "#         self.vocab_to_idx = {vocab: index for index, (vocab) in enumerate(self.vocabs)}\n",
    "#         self.vocab_to_idx['CLS'] = len(self.vocabs)\n",
    "#         self.vocab_to_idx['MASK'] = len(self.vocabs) + 1\n",
    "        \n",
    "#         for sequence in tqdm(sequences.split('\\n')):\n",
    "#             seq_idx = []\n",
    "#             seq_idx.append(self.vocab_to_idx['CLS'])\n",
    "#             for letter in sequence:\n",
    "#                 seq_idx.append(self.vocab_to_idx[letter])\n",
    "#             self.seq_idxes.append(torch.tensor(seq_idx, dtype=torch.int64))\n",
    "    \n",
    "#     def get_idx(self, worker_id):\n",
    "#         T = [list()] * self.num_workers\n",
    "#         for i, seq_idx in enumerate(self.seq_idxes):\n",
    "#             if i % self.num_workers == worker_id:\n",
    "#                 T[worker_id].append(seq_idx)\n",
    "#         return T[worker_id]\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.seq_idxes)\n",
    "    \n",
    "#     def __iter__(self):\n",
    "#         worker = data.get_worker_info()\n",
    "#         #print(worker.id)\n",
    "#         for i, seq_idx in enumerate(self.get_idx(worker.id)):\n",
    "#             #print(seq_idx)\n",
    "#             yield seq_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1b8e93-ec09-424c-a110-e6e52d11d172",
   "metadata": {},
   "source": [
    "#### Set Up Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b9224b6-437e-478c-8d01-cf42a3a038d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 524529/524529 [00:24<00:00, 21382.42it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ProtBERT(\n",
       "  (embedding): Embedding(27, 128)\n",
       "  (trans): TransformerBlock(\n",
       "    (mha): MultiHeadAttention(\n",
       "      (W_O): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (fc): Linear(in_features=128, out_features=25, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = f'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"using device: {device}\")\n",
    "batch_size = 1\n",
    "learning_rate = 0.01\n",
    "epochs = 2\n",
    "#filename = 'small_uniprot.txt'\n",
    "filename = 'uniprot-reviewed-lim_sequences.txt'\n",
    "d_model = 128\n",
    "n_head = 8\n",
    "num_workers = 4\n",
    "\n",
    "dataset = Sequences(filename)\n",
    "#dataset = IterableSequences(filename, num_workers)\n",
    "vocab_size = len(dataset.vocabs)\n",
    "model = ProtBERT(d_model, n_head, vocab_size, device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_loader = data.DataLoader(dataset=dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "model = model.to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b788a178-d0f3-4a58-a20c-464c746fe82f",
   "metadata": {},
   "source": [
    "#### Loop Through Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5695aa5-b339-4812-9fa0-cc310fd39152",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 524529/524529 [1:30:49<00:00, 96.25it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 3.055195, time: 90.825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 524529/524529 [1:30:47<00:00, 96.29it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Loss: 3.055758, time: 90.793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    sum_loss = 0\n",
    "    start = time.time()\n",
    "    for batch_idx, (X) in enumerate(tqdm(train_loader)):\n",
    "        # if batch_idx % 1000 == 0:\n",
    "        #     print(batch_idx)\n",
    "        X = X.to(device)\n",
    "        #print(X)\n",
    "        \n",
    "        # zero out prev gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        seq_len = len(X[0])\n",
    "        \n",
    "        # Random mask\n",
    "        M = torch.randint(1, seq_len, (int(seq_len * 0.15),))\n",
    "        \n",
    "        # feed the input into the model\n",
    "        y = model(X[0])\n",
    "        #print(y.size())\n",
    "        #print(y)\n",
    "        \n",
    "        # MASK the position\n",
    "        X1 = torch.clone(X[0])\n",
    "        X1[M] = dataset.vocab_to_idx['MASK']\n",
    "        #print(X1)\n",
    "        \n",
    "        # feed the masked input into the model\n",
    "        O = model(X1)\n",
    "        #print(O)\n",
    "        \n",
    "        # get all masked position O\n",
    "        #print(M)\n",
    "        O1 = O[M]\n",
    "        #print(O1.size())\n",
    "        \n",
    "        # get the true labels\n",
    "        true_label = X[0][M]\n",
    "        #print(true_label)\n",
    "        \n",
    "        # compute CE loss\n",
    "        loss = F.cross_entropy(torch.sigmoid(O1), true_label)\n",
    "        \n",
    "        # sum up batch losses\n",
    "        sum_loss += loss.item()\n",
    "        \n",
    "        # compute gradients and take a step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # average loss per example\n",
    "    sum_loss /= len(train_loader)\n",
    "    time_used = (time.time() - start) / 60\n",
    "    print(f'Epoch: {epoch}, Loss: {sum_loss:.6f}, time: {time_used:.3f}')\n",
    "    \n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'loss': sum_loss,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict()}\n",
    "    torch.save(checkpoint, 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddcc5ef-8f6e-4c21-ad86-3cfdb9dccfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "print(input)\n",
    "target = torch.tensor([0,1,0])\n",
    "print(target.reshape((-1, 1)))\n",
    "print(target.reshape((-1, 1))*input)\n",
    "output = loss(input, target)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f230ae-4dbb-41b3-95d5-5797d1daf006",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1,2],[3,4]], dtype=torch.float32)\n",
    "print(x)\n",
    "F.softmax(x, dim=1)\n",
    "\n",
    "x[torch.tensor([0,1]),:] = torch.tensor([5,6], dtype=torch.float32) \n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee77b0b5-3534-4da0-a04d-3f7b6ad98d71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
