{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02bea4be-cf23-4c67-9632-c7c15fef38cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4dbc0d-e9e8-4038-84d3-6e061ac1e987",
   "metadata": {},
   "source": [
    "#### Single Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78e3177f-4212-4e11-8e26-5057bfbf4b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleAttention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        \n",
    "        super(SingleAttention, self).__init__()\n",
    "        \n",
    "        self.d_k = int(d_model / 8)\n",
    "        \n",
    "        self.W_Q = nn.Linear(d_model, self.d_k)\n",
    "        self.W_K = nn.Linear(d_model, self.d_k)\n",
    "        self.W_V = nn.Linear(d_model, self.d_k)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        Q = self.W_Q(x)\n",
    "        K = self.W_K(x)\n",
    "        V = self.W_V(x)\n",
    "        \n",
    "        A = torch.matmul(Q, torch.transpose(K, 0, 1)) / torch.sqrt(torch.tensor(self.d_k))\n",
    "        \n",
    "        A = F.softmax(A, dim=1)\n",
    "        \n",
    "        V_prime = torch.matmul(A, V)\n",
    "        \n",
    "        return V_prime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db2b98f-20e3-42f2-8d20-7602df142f9e",
   "metadata": {},
   "source": [
    "#### Multi-head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b7962b1-0bad-420d-b0b5-b64acf21eb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_head, device):\n",
    "        \n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        self.d_k = int(d_model / 8)\n",
    "        self.n_head = n_head\n",
    "        \n",
    "        self.attentions = []\n",
    "        for i in range(self.n_head):\n",
    "            self.attentions.append(SingleAttention(d_model).to(device))\n",
    "        \n",
    "        self.W_O = nn.Linear(n_head * self.d_k, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        Vs = []\n",
    "        for i in range(self.n_head):\n",
    "            Vs.append(self.attentions[i](x))\n",
    "        \n",
    "        V = torch.cat(tuple(Vs), dim=1)\n",
    "        \n",
    "        x = self.W_O(V)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e760d8b3-84c0-4600-bbf3-bdff57e789c4",
   "metadata": {},
   "source": [
    "#### Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02f3606d-e418-4513-a6de-51b2a191adda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_head, device):\n",
    "        \n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "        self.mha = MultiHeadAttention(d_model, n_head, device).to(device)\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.mha(x)\n",
    "        x2 = self.ln1(x + x1)\n",
    "        x3 = self.fc(x2)\n",
    "        x4 = self.ln2(x3 + x2)\n",
    "        \n",
    "        return x4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f120f490-8283-41b7-b494-a98dca00cedf",
   "metadata": {},
   "source": [
    "#### BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cb81b9d-5636-47d2-981d-6de4448fdc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProtBERT(nn.Module):\n",
    "    def __init__(self, d_model, n_head, vocab_size, device):\n",
    "        \n",
    "        super(ProtBERT, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size+2, embedding_dim=d_model)\n",
    "        self.trans = TransformerBlock(d_model, n_head, device).to(device)\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        #print(x)\n",
    "        #x_embedding = torch.clone(x)\n",
    "        x = self.trans(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f067e0c-8da4-4f29-812a-214085c9540b",
   "metadata": {},
   "source": [
    "#### Define Dataset (Map Style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "537fb6d1-1b17-4eeb-9da0-552cf6d8978f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequences(torch.utils.data.Dataset):\n",
    "    def __init__(self, filename):\n",
    "        \n",
    "        self.seq_idxes = []  # set of sequence indexs\n",
    "        \n",
    "        file = open(filename, \"r\")\n",
    "        \n",
    "        while(True):\n",
    "            line = file.readline()\n",
    "            if(line != 'sequence' and line[0] != '#'):\n",
    "                break\n",
    "        \n",
    "        sequences = file.read().rstrip()\n",
    "        self.vocabs = sorted(set(sequences.replace('\\n','')))\n",
    "        self.vocab_to_idx = {vocab: index for index, (vocab) in enumerate(self.vocabs)}\n",
    "        self.vocab_to_idx['CLS'] = len(self.vocabs)\n",
    "        self.vocab_to_idx['MASK'] = len(self.vocabs) + 1\n",
    "        \n",
    "        for sequence in tqdm(sequences.split('\\n')):\n",
    "            seq_idx = []\n",
    "            seq_idx.append(self.vocab_to_idx['CLS'])\n",
    "            for letter in sequence:\n",
    "                seq_idx.append(self.vocab_to_idx[letter])\n",
    "            self.seq_idxes.append(torch.tensor(seq_idx, dtype=torch.int64))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.seq_idxes)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.seq_idxes[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e087a435-92d0-4352-96e6-1053c1510347",
   "metadata": {},
   "source": [
    "#### Set Up Loading and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca278163-3c76-43e3-a043-4a199024d587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 524529/524529 [00:24<00:00, 21104.38it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ProtBERT(\n",
       "  (embedding): Embedding(27, 128)\n",
       "  (trans): TransformerBlock(\n",
       "    (mha): MultiHeadAttention(\n",
       "      (W_O): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (fc): Linear(in_features=128, out_features=25, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = f'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"using device: {device}\")\n",
    "batch_size = 1\n",
    "learning_rate = 0.01\n",
    "epochs = 4\n",
    "#filename = 'small_uniprot.txt'\n",
    "filename = 'uniprot-reviewed-lim_sequences.txt'\n",
    "d_model = 128\n",
    "n_head = 8\n",
    "num_workers = 4\n",
    "dataset = Sequences(filename)\n",
    "vocab_size = len(dataset.vocabs)\n",
    "model = ProtBERT(d_model, n_head, vocab_size, device).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_loader = data.DataLoader(dataset=dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "checkpoint = torch.load('checkpoint.pth')\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2cf0d7-7beb-4750-94a4-79b40d80d09e",
   "metadata": {},
   "source": [
    "#### Loop Through Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20418e15-7e67-4ac3-89c9-a84848ddab3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 524529/524529 [1:28:48<00:00, 98.44it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Loss: 3.055744, time: 88.803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 524529/524529 [1:28:38<00:00, 98.62it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Loss: 3.055800, time: 88.641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 332486/524529 [55:56<32:21, 98.89it/s]   "
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch + 1, epochs + 1):\n",
    "    sum_loss = 0\n",
    "    start = time.time()\n",
    "    for batch_idx, (X) in enumerate(tqdm(train_loader)):\n",
    "        # if batch_idx % 1000 == 0:\n",
    "        #     print(batch_idx)\n",
    "        X = X.to(device)\n",
    "        #print(X)\n",
    "        \n",
    "        # zero out prev gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        seq_len = len(X[0])\n",
    "        \n",
    "        # Random mask\n",
    "        M = torch.randint(1, seq_len, (int(seq_len * 0.15),))\n",
    "        \n",
    "        # feed the input into the model\n",
    "        y = model(X[0])\n",
    "        #print(y.size())\n",
    "        #print(y)\n",
    "        \n",
    "        # MASK the position\n",
    "        X1 = torch.clone(X[0])\n",
    "        X1[M] = dataset.vocab_to_idx['MASK']\n",
    "        #print(X1)\n",
    "        \n",
    "        # feed the masked input into the model\n",
    "        O = model(X1)\n",
    "        #print(O)\n",
    "        \n",
    "        # get all masked position O\n",
    "        #print(M)\n",
    "        O1 = O[M]\n",
    "        #print(O1.size())\n",
    "        \n",
    "        # get the true labels\n",
    "        true_label = X[0][M]\n",
    "        #print(true_label)\n",
    "        \n",
    "        # compute CE loss\n",
    "        loss = F.cross_entropy(torch.sigmoid(O1), true_label)\n",
    "        \n",
    "        # sum up batch losses\n",
    "        sum_loss += loss.item()\n",
    "        \n",
    "        # compute gradients and take a step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # average loss per example\n",
    "    sum_loss /= len(train_loader)\n",
    "    time_used = (time.time() - start) / 60\n",
    "    print(f'Epoch: {epoch}, Loss: {sum_loss:.6f}, time: {time_used:.3f}')\n",
    "    \n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'loss': sum_loss,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict()}\n",
    "    torch.save(checkpoint, 'checkpoint2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b0a814-4d08-4ad9-b9cc-3d10050d4e85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
