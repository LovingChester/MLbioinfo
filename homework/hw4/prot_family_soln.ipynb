{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7fe9bab-f04b-4362-8298-9094abeca8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import torch.utils.data as du\n",
    "import torch.optim as optim\n",
    "import einops\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23322b34-e6e5-41cd-b86f-fd2ec5c74800",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequences:\n",
    "    '''\n",
    "    This class reads in the sequences, their family id, and \n",
    "    the protvec embeddings (one per sequence)\n",
    "    '''\n",
    "    def __init__(self, seq_fname, label_fname, protvec_fname,\n",
    "                 block_size, alphabet_idx, seed=42):\n",
    "        '''seq_fname: input sequence file -- family_classification_sequences.csv\n",
    "           label_fname: family id info -- family_classification_metadata.tab\n",
    "           protvec_fname: protvec file -- family_classification_protVec.csv\n",
    "           block_size: the sequence length used for bert training (1000)\n",
    "           alphabet_idx: the mapping from alphabet to token ids\n",
    "           seed: random seed for determinsitic data generation'''\n",
    "        \n",
    "        super(Sequences, self).__init__()\n",
    "             \n",
    "        np.random.seed(seed) # set the random seed\n",
    "        \n",
    "        self.alphabet_idx = alphabet_idx\n",
    "        self.block_size = block_size\n",
    "        \n",
    "        self.sequences = []  # set of sequences\n",
    "        self.labels = [] # family ids\n",
    "        self.protVec = [] # protvecs \n",
    "        \n",
    "        '''read protvecs, one per sequence'''\n",
    "        with open(protvec_fname, \"r\") as f:\n",
    "            for i, line in enumerate(f.readlines()):\n",
    "                a = line.strip().split(',')\n",
    "                self.protVec.append([float(v) for v in a])\n",
    "        self.protVec = np.array(self.protVec, dtype=np.float32)\n",
    "        \n",
    "        '''read protein sequences from file''' \n",
    "        with open(seq_fname, \"r\") as f:\n",
    "            for i, line in enumerate(f.readlines()):\n",
    "                a = line.strip()\n",
    "                if i == 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    #truncate sequences to block_size-1\n",
    "                    #since first token is CLS\n",
    "                    self.sequences.append(a[:block_size-1])\n",
    "                    \n",
    "        '''read family ids'''\n",
    "        with open(label_fname, \"r\") as f:\n",
    "            for i, line in enumerate(f.readlines()):\n",
    "                a = line.strip()\n",
    "                if i == 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    a = a.split(\"\\t\")\n",
    "                    self.labels.append(a[3].strip('\"'))\n",
    "        self.labels = np.array(self.labels)\n",
    "        \n",
    "        # sort families by frequency\n",
    "        fam_cnt = defaultdict(int)\n",
    "        for v in self.labels:\n",
    "            fam_cnt[v] += 1\n",
    "        self.fam_lst = sorted(fam_cnt.items(), \n",
    "                              reverse=True, key=lambda item: item[1])\n",
    "        \n",
    "    def get_fam(self, fam_id):\n",
    "        '''return all indices belonging to fam_id'''\n",
    "        idxs = np.where(self.labels == fam_id)[0]\n",
    "        return idxs\n",
    "    \n",
    "    def get_neg_fam(self, fam_id, seed):\n",
    "        '''sample negative instances for fam_id\n",
    "           return indices of negative class sequences'''\n",
    "        np.random.seed(seed) # set the random seed\n",
    "        pos_idxs = np.where(self.labels == fam_id)[0]\n",
    "        neg_idxs = np.random.choice(len(self.sequences), len(pos_idxs))\n",
    "        # make sure there is no overlap between pos and neg classes\n",
    "        neg_idxs = [idx for idx in neg_idxs if idx not in pos_idxs]\n",
    "        return neg_idxs\n",
    "    \n",
    "    def tokenize_and_pad(self, idx):\n",
    "        '''return tokenized sequence at idx\n",
    "        pad it if necessary to be block_size length\n",
    "        assumes 'PAD', 'CLS' are the tokens from BERT training'''\n",
    "        tokenized_seq = [self.alphabet_idx['CLS']]\n",
    "        #actual AA sequence\n",
    "        S = self.sequences[idx]\n",
    "        tokenized_seq.extend([self.alphabet_idx[S[i]]  \n",
    "                              if S[i] in self.alphabet_idx \n",
    "                              else self.alphabet_idx['PAD']\n",
    "                              for i in range(len(S))])\n",
    "        #PAD as remaining elements\n",
    "        pad_len = self.block_size - len(tokenized_seq)\n",
    "        tokenized_seq.extend([self.alphabet_idx['PAD'] \n",
    "                              for i in range(pad_len)])\n",
    "        return tokenized_seq\n",
    "    \n",
    "    def get_protvec(self, idx):\n",
    "        '''return protvec at idx'''\n",
    "        return self.protVec[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "accfe019-9815-4d95-aa11-4fdf927d3887",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fam_Dataset(Dataset):\n",
    "    '''\n",
    "        This class creates a dataset for the given fam_id.\n",
    "        getitem returns the CLS vector, protVec vector and label per idx\n",
    "    '''\n",
    "    def __init__(self, S, fam_id, transformer, device, seed=42):\n",
    "        '''S: an instance of the Sequences class\n",
    "           fam_id: create a dataset for fam_id (incude pos & neg instances)\n",
    "           transformer: transformer model (to extract CLS embeddings)\n",
    "        '''\n",
    "        super(Fam_Dataset, self).__init__()\n",
    "\n",
    "        self.seed = seed\n",
    "        self.X = [] # tokenized sequences for transformer\n",
    "        self.V = [] # protvec embeddings\n",
    "        self.y = [] # 1/0 for pos/neg class label per sequence\n",
    "        \n",
    "        # positive instances\n",
    "        fam_idx = S.get_fam(fam_id)       \n",
    "        for idx in fam_idx:\n",
    "            x = torch.tensor(S.tokenize_and_pad(idx))\n",
    "            x = x.unsqueeze(0)\n",
    "            x = x.to(device)\n",
    "            x = transformer(x)\n",
    "            # extract the embedding for CLS token at pos 0\n",
    "            x = torch.squeeze(x[:,0,:])\n",
    "            \n",
    "            self.X.append(x.cpu().numpy())\n",
    "            self.V.append(S.get_protvec(idx))\n",
    "            self.y.append(1.)\n",
    "        self.pos_sz = len(fam_idx)\n",
    "\n",
    "        # negative instances\n",
    "        neg_fam_idx = S.get_neg_fam(fam_id, seed=self.seed)\n",
    "        #print(\"NEG\", neg_fam_idx[:5])\n",
    "        for idx in neg_fam_idx:\n",
    "            x = torch.tensor(S.tokenize_and_pad(idx))\n",
    "            x = x.unsqueeze(0)\n",
    "            x = x.to(device)\n",
    "            x = transformer(x)\n",
    "            # extract the embedding for CLS token at pos 0\n",
    "            x = torch.squeeze(x[:,0,:])\n",
    "            self.X.append(x.cpu().numpy())\n",
    "            self.V.append(S.get_protvec(idx))\n",
    "            self.y.append(0.)\n",
    "        self.neg_sz = len(neg_fam_idx)\n",
    "        \n",
    "        self.X = np.array(self.X)\n",
    "        self.V = np.array(self.V)\n",
    "        self.y = np.array(self.y)\n",
    "            \n",
    "    def __len__(self):\n",
    "        '''return len of dataset'''\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''return X, V, y at idx'''\n",
    "        return self.X[idx], self.V[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dc77c30-fa28-4efd-8098-fa5587b4ed95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    '''Self Attention'''\n",
    "\n",
    "    def __init__(self, d, dk):\n",
    "        '''define WQ, WK, WV projection matrices:\n",
    "        d: d_model is the original model dimension\n",
    "        dk: projection dimension for query, keys and values\n",
    "        '''\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.d = d  # d_model\n",
    "        self.dk = dk  # d_k: projection dimension\n",
    "        self.WQ = nn.Linear(self.d, self.dk, bias=False)\n",
    "        self.WK = nn.Linear(self.d, self.dk, bias=False)\n",
    "        self.WV = nn.Linear(self.d, self.dk, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''project the context onto key, query and value spaces and\n",
    "        return the final value vectors\n",
    "        '''\n",
    "        # input shape: (batch_size, block_size, d)\n",
    "        # let batch_size=b, block_size=l, num_heads=h\n",
    "        Q = self.WQ(x)  # shape: b, l, dk\n",
    "        K = self.WK(x)  # shape: b, l, dk\n",
    "        V = self.WV(x)  # shape: b, l, dk\n",
    "\n",
    "        K = torch.transpose(K, 1, 2)  # K.T transpose\n",
    "        QKT = torch.bmm(Q, K)  # shape: b, l, l\n",
    "\n",
    "        # attention matrix\n",
    "        # row specifies weights for the value vectors, row add up to one\n",
    "        A = F.softmax(QKT / np.sqrt(self.dk), dim=2)  # shape: b, l, l\n",
    "\n",
    "        V = torch.bmm(A, V)  # shape: b, l, dk\n",
    "        return V\n",
    "\n",
    "\n",
    "class SepHeads_SelfAttention(nn.Module):\n",
    "    '''Separate Headed Self Attention: List of Attention Heads\n",
    "    This is a straightforward implementation of the multiple heads.\n",
    "    We have separate WQ, WK and WV matrices, one per head.'''\n",
    "\n",
    "    def __init__(self, d, dk, num_heads):\n",
    "        '''create separate heads:\n",
    "        d: d_model dimension\n",
    "        dk: projection dimension for query, keys and values\n",
    "        num_heads: number of attention heads\n",
    "        '''\n",
    "        super(SepHeads_SelfAttention, self).__init__()\n",
    "        self.d = d  # d_model\n",
    "        self.dk = dk  # d_k: projection dimension\n",
    "        self.num_heads = num_heads  # number of attention heads\n",
    "\n",
    "        self.sa_layers = nn.ModuleList()\n",
    "        for i in range(self.num_heads):\n",
    "            self.sa_layers.append(SelfAttention(self.d, self.dk))\n",
    "\n",
    "        self.WO = nn.Linear(self.dk * self.num_heads, self.d, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''use separate attention heads, and concat values'''\n",
    "        # input shape: (batch_size, block_size, d)\n",
    "        # let batch_size=b, block_size=l, num_heads=h\n",
    "        V = []\n",
    "        for i in range(self.num_heads):\n",
    "            V.append(self.sa_layers[i](x))\n",
    "\n",
    "        # concat all the value vectors from the heads\n",
    "        V = torch.cat(V, dim=2)  # shape: b, l, h x dk\n",
    "        # project back to d_model\n",
    "        x = self.WO(V)  # shape: b, l, d\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiHead_SelfAttention(nn.Module):\n",
    "    '''Multi Headed Self Attention:\n",
    "    Instead of using a list of attention heads with separate WQ, WK, WV matrices,\n",
    "    we combine all heads into one, and use a single WQ, WK and WV matrix.\n",
    "    Each matrix maps the d-dim input block into h*dk dim space, where h is num_heads.\n",
    "    We have to carefully keep the heads separate for softmax to achieve the same\n",
    "    effect at from the list of heads. We do that via einops and the very useful\n",
    "    torch.einsum function.\n",
    "    \n",
    "    This function is much more efficient than using separate heads.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, d, dk, num_heads):\n",
    "        '''create multi-heads -- joint heads:\n",
    "        d: d_model dimension\n",
    "        dk: projection dimension for query, keys and values\n",
    "        num_heads: number of attention heads\n",
    "        '''\n",
    "        super(MultiHead_SelfAttention, self).__init__()\n",
    "        self.d = d  # d_model\n",
    "        self.dk = dk  # d_k: projection dimension\n",
    "        self.num_heads = num_heads  # number of attention heads\n",
    "\n",
    "        self.WQ = nn.Linear(self.d, self.dk * self.num_heads, bias=False)\n",
    "        self.WK = nn.Linear(self.d, self.dk * self.num_heads, bias=False)\n",
    "        self.WV = nn.Linear(self.d, self.dk * self.num_heads, bias=False)\n",
    "        self.WO = nn.Linear(self.dk * self.num_heads, self.d, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input shape: (batch_size, block_size, d)\n",
    "        # let batch_size=b, block_size=l, num_heads=h, d_model=d\n",
    "        Q = self.WQ(x)  # size: (b, l, h*dk)\n",
    "        K = self.WK(x)  # size: (b, l, h*dk)\n",
    "        V = self.WV(x)  # size: (b, l, h*dk)\n",
    "\n",
    "        # split Q, K, V into heads and dk, move heads up front; KT is transpose of K\n",
    "        Q = einops.rearrange(\n",
    "            Q, 'b l (h dk)-> b h l dk', h=self.num_heads\n",
    "        )  # size: (b, h, l, dk)\n",
    "        KT = einops.rearrange(\n",
    "            K, 'b l (h dk)-> b h dk l', h=self.num_heads\n",
    "        )  # size: (b, h, dk, l)\n",
    "        V = einops.rearrange(\n",
    "            V, 'b l (h dk)-> b h l dk', h=self.num_heads\n",
    "        )  # size: (b, h, l, dk)\n",
    "\n",
    "        # compute Q x K.T, output is (b, h, l, l)\n",
    "        QKT = torch.einsum('bhik,bhkj->bhij', Q, KT)\n",
    "        A = F.softmax(QKT / np.sqrt(self.dk), dim=3)  # softmax along last dim\n",
    "\n",
    "        # new value representation\n",
    "        V = torch.einsum('bhik,bhkj->bhij', A, V)  # size: (b, h, l, dk)\n",
    "        V = einops.rearrange(V, 'b h l dk -> b l (h dk)')  # size: (b, l, h*dk)\n",
    "\n",
    "        # shape: b, l, h x dk\n",
    "        x = self.WO(V)  # shape: b, l, d\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    '''Transformer Block: multi-head or separate heads of attention,\n",
    "    followed by layernorm, ffn, and another layernorm\n",
    "    '''\n",
    "\n",
    "    def __init__(self, d, dk, num_heads, block_size, use_sepheads):\n",
    "        '''\n",
    "        d: d_model dimension\n",
    "        dk: projection dimension\n",
    "        num_heads: number of attention heads\n",
    "        use_sepheads: use separate heads or multiheads,\n",
    "                      multiheads is much more efficient\n",
    "        '''\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.use_sepheads = use_sepheads\n",
    "        self.drop_prob = 0.1\n",
    "\n",
    "        if self.use_sepheads:\n",
    "            # uses for loop for separate heads\n",
    "            self.mhsa = SepHeads_SelfAttention(d, dk, num_heads)\n",
    "        else:\n",
    "            # this is more efficient\n",
    "            self.mhsa = MultiHead_SelfAttention(d, dk, num_heads)\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(d)  # layer norm\n",
    "        self.ffn = nn.Sequential( #FFN module\n",
    "            nn.Linear(d, d),  # linear layer\n",
    "            nn.ReLU(),  # relu\n",
    "            nn.Linear(d, d)  # linear layer\n",
    "        )\n",
    "        self.ln2 = nn.LayerNorm(d)  # layer norm\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input shape: (batch_size, block_size, d)\n",
    "        # let batch_size=b, block_size=l, num_heads=h, d_model=d\n",
    "        x_sa = self.mhsa(x)  # multiple attention heads\n",
    "        x_sa = F.dropout(x_sa, p=self.drop_prob)\n",
    "        x_ln1 = self.ln1(x + x_sa)  # residual layer + layer norm\n",
    "        # two linear layers with relu in between\n",
    "        x_ffn = self.ffn(x_ln1)\n",
    "        x_ffn = F.dropout(x_ffn, p=self.drop_prob)\n",
    "        x_ln2 = self.ln2(x_ln1 + x_ffn)  # residual layer + layer norm\n",
    "        return x_ln2\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    '''Transformer model:\n",
    "    input is a block of tokens: first token is always CLS\n",
    "    MASK token for positions for training the masked language model\n",
    "    PAD tokens at end for sequences shorter than block size'''\n",
    "\n",
    "    def __init__(\n",
    "        self, d, dk, block_size, num_layers, num_heads, alphabet_idx, use_sepheads\n",
    "    ):\n",
    "        '''\n",
    "        d: d_model dimension\n",
    "        dk: projection dimension\n",
    "        block_size: the max sequence length\n",
    "        num_layers: how many transformer blocks/layers?\n",
    "        num_heads: number of attention heads\n",
    "        alphabet_idx: dict of tokens to token ids (ints)\n",
    "        use_sepheads: use separate heads or joint heads (multiheads),\n",
    "                      multiheads is much more efficient\n",
    "\n",
    "        '''\n",
    "        super(Transformer, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.drop_prob = 0.1  # for dropout layer\n",
    "\n",
    "        # embedding layer to map tokens to d dim vectors\n",
    "        self.embed = nn.Embedding(len(alphabet_idx), d, padding_idx=alphabet_idx['PAD'])\n",
    "\n",
    "        # learnable position embeddings, one per sequence element in block\n",
    "        # can also use sine/cosine embeddings: not done here!\n",
    "        self.pos_embed = nn.Embedding(block_size, d)\n",
    "\n",
    "        # list of transformer blocks/layers\n",
    "        tb_list = [\n",
    "            TransformerBlock(d, dk, num_heads, block_size, use_sepheads)\n",
    "            for i in range(self.num_layers)\n",
    "        ]\n",
    "        # combine all layers into one \"sequential\" layer\n",
    "        self.layers = nn.Sequential(*tb_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input shape: batch_size (b), block_size (l)\n",
    "        # d is d_model\n",
    "        p = self.pos_embed.weight  # shape: l, d\n",
    "        x = self.embed(x) + p  # add pos embeddings, shape: b, l, d\n",
    "        x = F.dropout(x, p=self.drop_prob)  # dropout\n",
    "        x = self.layers(x)  # shape: (b, l, d)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c0e32c6-ced4-4119-96bc-07d0d6697641",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, dropout):\n",
    "        '''in_dim: input layer dim\n",
    "           hidden_dim: hidden layer dim\n",
    "           out_dim: output layer dim\n",
    "           dropout: dropout probability\n",
    "           '''\n",
    "        \n",
    "        super(MLP, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        #two fully connected layers\n",
    "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # compute output of fc1, and apply relu activation, followed by dropout\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, p=self.dropout)           \n",
    "       # compute output layer\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eef0b0fa-e7a9-4902-a980-1a397099e613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(indim, batch_size, learning_rate, use_protvec):\n",
    "    model = MLP(indim, 64, 1, dropout)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate,\n",
    "                          weight_decay=weight_decay)\n",
    "\n",
    "    # load training and validation data in batches\n",
    "    data_loader = du.DataLoader(fam_data,\n",
    "                                 batch_size=batch_size,\n",
    "                                 shuffle=True)\n",
    "\n",
    "    # send model over to device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        sum_loss = 0.\n",
    "        correct = 0.\n",
    "        for batch_idx, (data, protvec, target) in enumerate(data_loader):\n",
    "            # send batch over to device\n",
    "            data = data.to(device)\n",
    "            protvec = protvec.to(device)\n",
    "            target = target.to(device)\n",
    "            # zero out prev gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # run the forward pass\n",
    "            if use_protvec:\n",
    "                output = model(protvec).flatten()\n",
    "            else:\n",
    "                output = model(data).flatten()\n",
    "            # compute loss/error with weight per sample\n",
    "            loss = F.binary_cross_entropy_with_logits(\n",
    "                    output, target)\n",
    "            sum_loss += loss.item()\n",
    "\n",
    "            #compute training accuracy\n",
    "            correct += compute_correct(output, target)\n",
    "\n",
    "            # compute gradients and take a step\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # average loss per example\n",
    "        sum_loss /= (batch_idx+1)\n",
    "        train_acc = correct / len(fam_data)\n",
    "        #print(f'Epoch: {epoch}, Loss: {sum_loss:.6e}, Acc: {train_acc:.4f}') \n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48f32d7d-e5cf-435d-a758-712c846868c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_correct(output, target):\n",
    "    '''first apply sigmoid and predict class 1 if >= 0.5, 0 otherwise\n",
    "    '''\n",
    "    #use logsigmoid for log space computations\n",
    "    output = F.logsigmoid(output.detach())\n",
    "    pred = torch.where(output > F.logsigmoid(torch.tensor(0.5)), \n",
    "                       1, 0)\n",
    "\n",
    "    # add up weights of correct predictions\n",
    "    correct = torch.sum(pred == target)\n",
    "    \n",
    "    return correct.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d2f0e95-7055-4181-811f-fecb3a560c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (embed): Embedding(28, 256, padding_idx=27)\n",
       "  (pos_embed): Embedding(1000, 256)\n",
       "  (layers): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (mhsa): MultiHead_SelfAttention(\n",
       "        (WQ): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (WK): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (WV): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (WO): Linear(in_features=256, out_features=256, bias=False)\n",
       "      )\n",
       "      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffn): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (mhsa): MultiHead_SelfAttention(\n",
       "        (WQ): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (WK): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (WV): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (WO): Linear(in_features=256, out_features=256, bias=False)\n",
       "      )\n",
       "      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffn): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = f'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"using device: {device}\")\n",
    "model_fname = 'transformer_d256_dk32_l2_h8_lr0.0001_e10_j1700463.pth'\n",
    "saveinfo = torch.load(model_fname)\n",
    "d = saveinfo['d']\n",
    "dk = saveinfo['dk']\n",
    "num_layers = saveinfo['l']\n",
    "num_heads = saveinfo['h']\n",
    "block_size = saveinfo['block_size']\n",
    "alphabet_idx = saveinfo['alphabet_idx']\n",
    "use_sepheads=False\n",
    "\n",
    "transformer = Transformer(d, dk, block_size, num_layers, \n",
    "                    num_heads, alphabet_idx, use_sepheads)\n",
    "transformer.load_state_dict(saveinfo['model'])\n",
    "\n",
    "#freeze the model\n",
    "for param in transformer.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "transformer.to(device)\n",
    "transformer.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0d579e7-3838-4340-9d49-42cfec014d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = Sequences('family_classification_sequences.csv', \n",
    "              'family_classification_metadata.tab', \n",
    "              'family_classification_protVec.csv',\n",
    "              block_size, alphabet_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7bad8eff-c9db-48ea-b1db-4018e9945e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC MMR_HSR1 0.8547677261613692 0.9426242868785656 10.541239261627197 17.729961395263672\n",
      "ACC Helicase_C 0.8875598086124402 0.6915869218500797 13.924832105636597 19.231462240219116\n",
      "ACC ATP-synt_ab 0.9354092152324848 0.9671786240269303 8.295345783233643 13.140318870544434\n",
      "ACC 7tm_1 0.9387417218543046 0.8887969094922737 6.207003593444824 9.908357381820679\n",
      "ACC AA_kinase 0.8973623853211009 0.8864678899082569 6.003232717514038 9.422768592834473\n",
      "ACC AAA 0.8535871156661786 0.8459736456808199 6.043012619018555 9.510087490081787\n",
      "ACC tRNA-synt_1 0.9671879791475008 0.9546151487273843 5.704678773880005 8.96895456314087\n",
      "ACC tRNA-synt_2 0.8684303350970017 0.8719576719576719 4.953840017318726 7.712321519851685\n",
      "ACC MFS_1 0.9546153846153846 0.936923076923077 4.522503614425659 7.1766767501831055\n",
      "ACC HSP70 0.9376971608832808 0.9684542586750788 4.395656108856201 6.82806921005249\n",
      "ACC Oxidored_q1 0.9759711653984782 0.9763716459751702 4.362226247787476 6.778350830078125\n",
      "ACC His_biosynth 0.9405861099959856 0.9421918908069049 4.31998872756958 6.730767250061035\n",
      "ACC Cpn60_TCP1 0.9465004022526147 0.9718423169750603 4.289921045303345 6.710888624191284\n",
      "ACC EPSP_synthase 0.9245126503525508 0.9514724180837827 4.217970609664917 6.727104187011719\n",
      "ACC Aldedh 0.9039264828738512 0.8964076858813701 4.184977769851685 6.5250022411346436\n",
      "ACC Shikimate_DH 0.8306666666666667 0.8284444444444444 3.9019062519073486 6.16976523399353\n",
      "ACC GHMP_kinases_N 0.8671735241502684 0.7965116279069767 3.9063384532928467 6.173623323440552\n",
      "ACC Ribosomal_S2 0.9088385006941231 0.967144840351689 3.717479944229126 5.969072580337524\n",
      "ACC Ribosomal_S4 0.9691588785046729 0.9663551401869159 3.6907408237457275 5.8318562507629395\n",
      "ACC Ribosomal_L16 0.9428571428571428 0.9542857142857143 3.6517462730407715 5.825852632522583\n",
      "total time 173.09419989585876\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "epochs = 50\n",
    "weight_decay = 0.\n",
    "dropout = 0.5\n",
    "d_protvec = 100\n",
    "    \n",
    "st = time.time()\n",
    "ACC = []\n",
    "for fidx, (fam_id, fam_cnt) in enumerate(S.fam_lst):\n",
    "    if fidx > 19:\n",
    "        break\n",
    "    st_time = time.time()\n",
    "    fam_data = Fam_Dataset(S, fam_id, transformer, device)\n",
    "    en_time = time.time()\n",
    "    fam_time = en_time - st_time\n",
    "    T_acc = train_model(d, batch_size, 0.001, use_protvec = False)\n",
    "    P_acc = train_model(d_protvec, batch_size, 0.01, use_protvec=True)\n",
    "    en_time = time.time()\n",
    "    print(\"ACC\", fam_id, T_acc, P_acc, fam_time, en_time-st_time)\n",
    "    ACC.append((fam_id, T_acc, P_acc))\n",
    "\n",
    "en = time.time()\n",
    "print(\"total time\", en-st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1ae2dc0-0124-4977-960b-76965f0a2e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC 0.9152775178168702 0.04191850109283851 0.9102803079509083 0.07204960254440099\n"
     ]
    }
   ],
   "source": [
    "Tacc = np.array([float(t) for (f,t,p) in ACC])\n",
    "Pacc = np.array([float(p) for (f,t,p) in ACC])\n",
    "#print(Tacc)\n",
    "print(\"ACC\", Tacc.mean(), Tacc.std(), Pacc.mean(), Pacc.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9919637f-9d18-41cb-943b-52f8f5caeaf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
